{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DL-xS3wo1LtM"
   },
   "source": [
    "# Importanto bibliotecas e Definindo diretórios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "MINx5gXN1Pfc"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from tqdm import tqdm\n",
    "\n",
    "data_dir = f\"./data\"\n",
    "#corel-1k\n",
    "dataset_dir = f\"{data_dir}/dataset\"\n",
    "images_dir = f\"{dataset_dir}/images\"\n",
    "#caltech256\n",
    "dataset_1_dir = f\"{data_dir}/dataset_1\"\n",
    "images_1_dir = f\"{dataset_1_dir}/images\"\n",
    "#cifar100\n",
    "dataset_2_dir = f\"{data_dir}/dataset_2\"\n",
    "images_2_dir = f\"{dataset_2_dir}/images\"\n",
    "#cifar10\n",
    "dataset_3_dir = f\"{data_dir}/dataset_3\"\n",
    "images_3_dir = f\"{dataset_3_dir}/images\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Funções p/ escala de cinza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geração de histogramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obter_vizinhos(matriz_de_intensidade, linha, coluna):\n",
    "    # O objetivo dessa função é retornar uma lista contendo a intensidade dos 9 vizinhos do pixel observado\n",
    "\n",
    "    #Esse método de partição da matriz foi escolhido para manter a analise dos pixels vizinhos\n",
    "    #dentro dos limites da matriz de intensidade, evitando assim valores negativos ou fora do shape.\n",
    "    vizinhos = matriz_de_intensidade[max(0, linha-1):min(matriz_de_intensidade.shape[0], linha+2),\n",
    "                             max(0, coluna-1):min(matriz_de_intensidade.shape[1], coluna+2)]\n",
    "\n",
    "    #Transforma a matriz particionada em lista e remove o pixel do centro\n",
    "    lista_de_vizinhos = vizinhos.flatten().tolist()\n",
    "    lista_de_vizinhos.remove(matriz_de_intensidade[linha, coluna])\n",
    "\n",
    "    return lista_de_vizinhos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aplica_regras(lista_de_vizinhos, intensidade_pixel_central, estado_pixel_central):\n",
    "    #O objetivo dessa função é aplicar as regras do jogo da vida de Conway no pixel observado e retornar seu estado.\n",
    "    # 1 = vivo, 0 = morto\n",
    "\n",
    "    #Conta o número de vizinhos vivos com a mesma intensidade que o pixel central\n",
    "    vizinhos_iguais = lista_de_vizinhos.count(intensidade_pixel_central)\n",
    "\n",
    "    #Regra 1: A célula viva com dois ou três vizinhos vivos sobrevive\n",
    "    if estado_pixel_central == 1 and (vizinhos_iguais == 2 or vizinhos_iguais == 3):\n",
    "        return 1\n",
    "\n",
    "    #Regra 2: A célula viva com menos de dois vizinhos vivos morre (subpopulação)\n",
    "    elif estado_pixel_central == 1 and vizinhos_iguais < 2:\n",
    "        return 0\n",
    "\n",
    "    #Regra 3: A célula viva com mais de três vizinhos vivos morre (superpopulação)\n",
    "    elif estado_pixel_central == 1 and vizinhos_iguais > 3:\n",
    "        return 0\n",
    "\n",
    "    #Regra 4: A célula morta com exatamente três vizinhos vivos se torna viva (resurreição)\n",
    "    elif estado_pixel_central == 0 and vizinhos_iguais == 3:\n",
    "        return 1\n",
    "\n",
    "    #Pra todos os outros casos, a célula permanece no mesmo estado\n",
    "    else:\n",
    "        return estado_pixel_central"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def percorre_imagem_aplicando_regras(matriz_de_estados, matriz_de_intensidade):\n",
    "    # O objetivo dessa função é percorrer a imagem, chamar a função para obter os vizinhos e aplicar as regras\n",
    "\n",
    "    linhas, colunas = matriz_de_intensidade.shape\n",
    "    for linha in range(linhas):\n",
    "        for coluna in range(colunas):\n",
    "            #Obtem os vizinhos do pixel atual\n",
    "            lista_de_vizinhos = obter_vizinhos(matriz_de_intensidade, linha, coluna)\n",
    "            #Aplica as regras do jogo da vida no pixel atual (atualiza a matriz de estado inicial)\n",
    "            matriz_de_estados[linha, coluna] = aplica_regras(lista_de_vizinhos, matriz_de_intensidade[linha, coluna], matriz_de_estados[linha, coluna])\n",
    "\n",
    "    return matriz_de_estados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gera_histogramas(imagem_cinza):\n",
    "    #O objetivo dessa função é criar as matrizes de intensidade e as de estado inicial para cada imagem\n",
    "    #Após aplicar as regras cria os histogramas\n",
    "\n",
    "    #Transforma a imagem em uma matriz de intensidade\n",
    "    matriz_de_intensidade = np.array(imagem_cinza)\n",
    "\n",
    "    #Cria as matrizes de estados iniciais\n",
    "    matriz_de_estados_phi = np.ones(matriz_de_intensidade.shape, dtype=int) #todos vivos\n",
    "    matriz_de_estados_psi = np.zeros(matriz_de_intensidade.shape, dtype=int) #todos mortos\n",
    "\n",
    "    #Aplica as regras do jogo da vida e atualiza as matrizes de estado inicial\n",
    "    matriz_de_estados_phi = percorre_imagem_aplicando_regras(matriz_de_estados_phi, matriz_de_intensidade)\n",
    "    matriz_de_estados_psi = percorre_imagem_aplicando_regras(matriz_de_estados_psi, matriz_de_intensidade)\n",
    "\n",
    "    #As matrizes são convertidas em listas\n",
    "    #Phi -> estado inicial = vivo\n",
    "    phi_vivos = matriz_de_intensidade.flatten()[matriz_de_estados_phi.flatten() == 1] #se manteram vivos\n",
    "    phi_mortos = matriz_de_intensidade.flatten()[matriz_de_estados_phi.flatten() == 0] #morreram\n",
    "\n",
    "    #Psi -> estado inicial = morto\n",
    "    psi_vivos = matriz_de_intensidade.flatten()[matriz_de_estados_psi.flatten() == 1] #ressuscitaram\n",
    "    psi_mortos = matriz_de_intensidade.flatten()[matriz_de_estados_psi.flatten() == 0] #se manteram mortos\n",
    "\n",
    "    #Cria os histogramas\n",
    "    hist_phi_vivos, _ = np.histogram(phi_vivos, bins=256, range=(0, 256))\n",
    "    hist_phi_mortos, _ = np.histogram(phi_mortos, bins=256, range=(0, 256))\n",
    "    hist_psi_vivos, _ = np.histogram(psi_vivos, bins=256, range=(0, 256))\n",
    "    hist_psi_mortos, _ = np.histogram(psi_mortos, bins=256, range=(0, 256))\n",
    "\n",
    "    return hist_phi_vivos, hist_phi_mortos, hist_psi_vivos, hist_psi_mortos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gerador_histogramas(dataset_dir, escala):\n",
    "    #Funcao principal: faz a chamada das funções acima e salva os histogramas\n",
    "\n",
    "    print(\"Gerando histogramas\")\n",
    "    images_dir = f\"{dataset_dir}/images\"\n",
    "    histograms_dir = f\"{dataset_dir}/histograms_grayscale\"\n",
    "\n",
    "    # Cria as pastas para salvar os histogramas mantendo o padrão das classes do dataset\n",
    "    os.makedirs(histograms_dir, exist_ok=True)\n",
    "    classes = [conteudo_item for conteudo_item in os.listdir(images_dir) if os.path.isdir(os.path.join(images_dir, conteudo_item))]\n",
    "    for classe in classes:\n",
    "        dir_pastas = os.path.join(histograms_dir, classe)\n",
    "        os.makedirs(dir_pastas, exist_ok=True)\n",
    "\n",
    "\n",
    "    #Esse loop pega a imagem, gera seus histogramas e salva com base no nome da classe e da imagem\n",
    "    for classe in os.listdir(images_dir):\n",
    "        dir_classe = f\"{images_dir}/{classe}\"\n",
    "        classe = f\"{classe}\"\n",
    "        \n",
    "        lista_arquivos = os.listdir(dir_classe)\n",
    "        num_arquivos = len(lista_arquivos)\n",
    "        \n",
    "        progress_bar = tqdm(total=num_arquivos, desc= classe)\n",
    "        \n",
    "        for imagem in os.listdir(dir_classe):\n",
    "            imagem_path = f\"{images_dir}/{classe}/{imagem}\"\n",
    "\n",
    "            imagem_cinza = cv2.imread(imagem_path, cv2.IMREAD_GRAYSCALE) #Lendo em escala de cinza\n",
    "            imagem_cinza = cv2.resize(imagem_cinza, (escala,escala)) #Padronizando tamanho das imagens\n",
    "            \n",
    "            hist_phi_vivos, hist_phi_mortos, hist_psi_vivos, hist_psi_mortos = gera_histogramas(imagem_cinza)\n",
    "\n",
    "            img_name = imagem.split(\".\")[0]\n",
    "            file_path = os.path.join(f\"{histograms_dir}/{classe}\", f\"{img_name}_phi_vivos.pkl\")\n",
    "            joblib.dump(hist_phi_vivos, file_path)\n",
    "            file_path = os.path.join(f\"{histograms_dir}/{classe}\", f\"{img_name}_phi_mortos.pkl\")\n",
    "            joblib.dump(hist_phi_mortos, file_path)\n",
    "            file_path = os.path.join(f\"{histograms_dir}/{classe}\", f\"{img_name}_psi_vivos.pkl\")\n",
    "            joblib.dump(hist_psi_vivos, file_path)\n",
    "            file_path = os.path.join(f\"{histograms_dir}/{classe}\", f\"{img_name}_psi_mortos.pkl\")\n",
    "            joblib.dump(hist_psi_mortos, file_path)\n",
    "            progress_bar.update(1)\n",
    "\n",
    "        progress_bar.close()\n",
    "\n",
    "    print(\"Progresso concluído\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparação de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def carrega_hist(imagem_path):\n",
    "    #O objetivo dessa função é carregar os histogramas de determinada imagem\n",
    "\n",
    "    #./data/dataset/images/classe/imagem.jpg\n",
    "    imagem_path = imagem_path.split(\"/\")\n",
    "    dataset, classe, imagem = imagem_path[2], imagem_path[4], imagem_path[5]\n",
    "    imagem = imagem.split(\".\")[0]\n",
    "\n",
    "    histogramas_dir = f\"./data/{dataset}/histograms_grayscale\"\n",
    "\n",
    "    try:\n",
    "        #Buscando os histogramas no diretorio\n",
    "        hist_phi_vivos = joblib.load(f\"{histogramas_dir}/{classe}/{imagem}_phi_vivos.pkl\")\n",
    "        hist_phi_mortos = joblib.load(f\"{histogramas_dir}/{classe}/{imagem}_phi_mortos.pkl\")\n",
    "        hist_psi_vivos = joblib.load(f\"{histogramas_dir}/{classe}/{imagem}_psi_vivos.pkl\")\n",
    "        hist_psi_mortos = joblib.load(f\"{histogramas_dir}/{classe}/{imagem}_psi_mortos.pkl\")\n",
    "\n",
    "        return hist_phi_vivos, hist_phi_mortos, hist_psi_vivos, hist_psi_mortos\n",
    "\n",
    "    except:\n",
    "        print(imagem_path)\n",
    "        print(\"Os histogramas não foram encontrados. Verifique os diretórios e se os histogramas foram criados corretamente.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retorna_combinacao(imagem_path, combinacao):\n",
    "    # O objetivo dessa funcao é criar combinacoes com os histogramas da imagem\n",
    "\n",
    "    hist_phi_vivos, hist_phi_mortos, hist_psi_vivos, hist_psi_mortos = carrega_hist(imagem_path)\n",
    "    if combinacao == 0:\n",
    "        return hist_phi_vivos\n",
    "    elif combinacao == 1:\n",
    "        return hist_phi_mortos\n",
    "    elif combinacao == 2:\n",
    "        return hist_psi_vivos\n",
    "    elif combinacao == 3:\n",
    "        return hist_psi_mortos\n",
    "    elif combinacao == 4:\n",
    "        return np.concatenate([hist_phi_vivos, hist_phi_mortos]) #histogramas phi\n",
    "    elif combinacao == 5:\n",
    "        return np.concatenate([hist_psi_vivos, hist_psi_mortos]) #histogramas psi\n",
    "    elif combinacao == 6:\n",
    "        return np.concatenate([hist_phi_vivos, hist_psi_vivos]) #histogramas vivos\n",
    "    elif combinacao == 7:\n",
    "        return np.concatenate([hist_phi_mortos, hist_psi_mortos]) #histogramas mortos\n",
    "    elif combinacao == 8:\n",
    "        return np.concatenate([np.concatenate([hist_phi_vivos, hist_phi_mortos]),\n",
    "                               np.concatenate([hist_psi_vivos, hist_psi_mortos])])\n",
    "        #histogramas phi e psi combinados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepara os dados para treinar o modelo\n",
    "def retorna_dados(images_dir):\n",
    "    histogramas = []\n",
    "    rotulos = []\n",
    "\n",
    "    print(\"Carregando histogramas\")\n",
    "    for classe in os.listdir(images_dir):\n",
    "        dir_classe = f\"{images_dir}/{classe}\"\n",
    "        classe = f\"{classe}\"\n",
    "        \n",
    "        lista_arquivos = os.listdir(dir_classe)\n",
    "        num_arquivos = len(lista_arquivos)\n",
    "        progress_bar = tqdm(total=num_arquivos, desc= classe)\n",
    "        \n",
    "        for imagem in os.listdir(dir_classe):\n",
    "            imagem_path = f\"{images_dir}/{classe}/{imagem}\"\n",
    "    \n",
    "            #Pode alterar a combinacao\n",
    "            combinacao = retorna_combinacao(imagem_path, 8)\n",
    "    \n",
    "            #Lista dos histogramas\n",
    "            histogramas.append(combinacao)\n",
    "            #lista da classe assossiada ao histograma\n",
    "            rotulos.append(classe)\n",
    "            progress_bar.update(1)\n",
    "        \n",
    "        progress_bar.close()\n",
    "        \n",
    "    print(\"Progresso concluído\")\n",
    "    return histogramas, rotulos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Funções p/ RGB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geração de histogramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obter_vizinhos_rgb(matriz_de_intensidade, linha, coluna):\n",
    "    # O objetivo dessa função é retornar uma lista contendo a intensidade dos 9 vizinhos do pixel observado\n",
    "\n",
    "    #Esse método de partição da matriz foi escolhido para manter a analise dos pixels vizinhos\n",
    "    #dentro dos limites da matriz de intensidade, evitando assim valores negativos ou fora do shape.\n",
    "    vizinhos = matriz_de_intensidade[max(0, linha-1):min(matriz_de_intensidade.shape[0], linha+2),\n",
    "                             max(0, coluna-1):min(matriz_de_intensidade.shape[1], coluna+2)]\n",
    "\n",
    "    #Transforma a matriz particionada em lista e remove o pixel do centro\n",
    "    lista_de_vizinhos = vizinhos.flatten().tolist()\n",
    "    lista_de_vizinhos.remove(matriz_de_intensidade[linha, coluna])\n",
    "\n",
    "    return lista_de_vizinhos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aplica_regras_rgb(lista_de_vizinhos, intensidade_pixel_central, estado_pixel_central):\n",
    "    #O objetivo dessa função é aplicar as regras do jogo da vida de Conway no pixel observado e retornar seu estado.\n",
    "    # 1 = vivo, 0 = morto\n",
    "\n",
    "    #Conta o número de vizinhos vivos com a mesma intensidade que o pixel central\n",
    "    vizinhos_iguais = lista_de_vizinhos.count(intensidade_pixel_central)\n",
    "\n",
    "    #Regra 1: A célula viva com dois ou três vizinhos vivos sobrevive\n",
    "    if estado_pixel_central == 1 and (vizinhos_iguais == 2 or vizinhos_iguais == 3):\n",
    "        return 1\n",
    "\n",
    "    #Regra 2: A célula viva com menos de dois vizinhos vivos morre (subpopulação)\n",
    "    elif estado_pixel_central == 1 and vizinhos_iguais < 2:\n",
    "        return 0\n",
    "\n",
    "    #Regra 3: A célula viva com mais de três vizinhos vivos morre (superpopulação)\n",
    "    elif estado_pixel_central == 1 and vizinhos_iguais > 3:\n",
    "        return 0\n",
    "\n",
    "    #Regra 4: A célula morta com exatamente três vizinhos vivos se torna viva (resurreição)\n",
    "    elif estado_pixel_central == 0 and vizinhos_iguais == 3:\n",
    "        return 1\n",
    "\n",
    "    #Pra todos os outros casos, a célula permanece no mesmo estado\n",
    "    else:\n",
    "        return estado_pixel_central"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def percorre_imagem_aplicando_regras_rgb(matriz_de_estados, matriz_de_intensidade):\n",
    "    # O objetivo dessa função é percorrer a imagem, chamar a função para obter os vizinhos e aplicar as regras\n",
    "\n",
    "    linhas, colunas = matriz_de_intensidade.shape\n",
    "    for linha in range(linhas):\n",
    "        for coluna in range(colunas):\n",
    "            #Obtem os vizinhos do pixel atual\n",
    "            lista_de_vizinhos = obter_vizinhos_rgb(matriz_de_intensidade, linha, coluna)\n",
    "            #Aplica as regras do jogo da vida no pixel atual (atualiza a matriz de estado inicial)\n",
    "            matriz_de_estados[linha, coluna] = aplica_regras_rgb(lista_de_vizinhos, matriz_de_intensidade[linha, coluna], matriz_de_estados[linha, coluna])\n",
    "\n",
    "    return matriz_de_estados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gera_histogramas_rgb(imagem):\n",
    "    #O objetivo dessa função é criar as matrizes de intensidade e as de estado inicial para cada imagem\n",
    "    #Após aplicar as regras cria os histogramas\n",
    "\n",
    "    #Transforma a imagem em uma matriz de intensidade\n",
    "    matriz_de_intensidade = np.array(imagem)\n",
    "\n",
    "    #Cria as matrizes de estados iniciais\n",
    "    matriz_de_estados_phi = np.ones(matriz_de_intensidade.shape, dtype=int) #todos vivos\n",
    "    matriz_de_estados_psi = np.zeros(matriz_de_intensidade.shape, dtype=int) #todos mortos\n",
    "\n",
    "    #Aplica as regras do jogo da vida e atualiza as matrizes de estado inicial\n",
    "    matriz_de_estados_phi = percorre_imagem_aplicando_regras_rgb(matriz_de_estados_phi, matriz_de_intensidade)\n",
    "    matriz_de_estados_psi = percorre_imagem_aplicando_regras_rgb(matriz_de_estados_psi, matriz_de_intensidade)\n",
    "\n",
    "    #As matrizes são convertidas em listas\n",
    "    #Phi -> estado inicial = vivo\n",
    "    phi_vivos = matriz_de_intensidade.flatten()[matriz_de_estados_phi.flatten() == 1] #se manteram vivos\n",
    "    phi_mortos = matriz_de_intensidade.flatten()[matriz_de_estados_phi.flatten() == 0] #morreram\n",
    "\n",
    "    #Psi -> estado inicial = morto\n",
    "    psi_vivos = matriz_de_intensidade.flatten()[matriz_de_estados_psi.flatten() == 1] #ressuscitaram\n",
    "    psi_mortos = matriz_de_intensidade.flatten()[matriz_de_estados_psi.flatten() == 0] #se manteram mortos\n",
    "\n",
    "    #Cria os histogramas\n",
    "    hist_phi_vivos, _ = np.histogram(phi_vivos, bins=256, range=(0, 256))\n",
    "    hist_phi_mortos, _ = np.histogram(phi_mortos, bins=256, range=(0, 256))\n",
    "    hist_psi_vivos, _ = np.histogram(psi_vivos, bins=256, range=(0, 256))\n",
    "    hist_psi_mortos, _ = np.histogram(psi_mortos, bins=256, range=(0, 256))\n",
    "\n",
    "    return hist_phi_vivos, hist_phi_mortos, hist_psi_vivos, hist_psi_mortos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gerador_histogramas_RGB(dataset_dir, escala):\n",
    "    images_dir = f\"{dataset_dir}/images\"\n",
    "    histograms_dir = f\"{dataset_dir}/histograms_rgb\"\n",
    "\n",
    "    print(\"Gerando histogramas\")\n",
    "    #Criando diretorios das classes\n",
    "    os.makedirs(histograms_dir, exist_ok=True)\n",
    "    classes = [d for d in os.listdir(images_dir) if os.path.isdir(os.path.join(images_dir, d))]\n",
    "    for classe in classes:\n",
    "        os.makedirs(os.path.join(histograms_dir, classe), exist_ok=True)\n",
    "\n",
    "    for classe in classes:\n",
    "        dir_classe = os.path.join(images_dir, classe)\n",
    "\n",
    "        lista_arquivos = os.listdir(dir_classe)\n",
    "        num_arquivos = len(lista_arquivos)\n",
    "        progress_bar = tqdm(total=num_arquivos, desc= classe)\n",
    "        \n",
    "        for imagem in os.listdir(dir_classe):\n",
    "            imagem_path = os.path.join(dir_classe, imagem)\n",
    "            imagem_rgb = cv2.imread(imagem_path, cv2.IMREAD_COLOR) #Lendo a imagem em BGR\n",
    "            imagem_rgb = cv2.resize(imagem_rgb, (escala, escala))\n",
    "\n",
    "            #Gera os histogramas para cada canal da imagem\n",
    "            hist_phi_vivos_b, hist_phi_mortos_b, hist_psi_vivos_b, hist_psi_mortos_b = gera_histogramas_rgb(imagem_rgb[:, :, 0])\n",
    "            hist_phi_vivos_g, hist_phi_mortos_g, hist_psi_vivos_g, hist_psi_mortos_g = gera_histogramas_rgb(imagem_rgb[:, :, 1])\n",
    "            hist_phi_vivos_r, hist_phi_mortos_r, hist_psi_vivos_r, hist_psi_mortos_r = gera_histogramas_rgb(imagem_rgb[:, :, 2])\n",
    "\n",
    "            #Concatena os histogramas dos 3 canais para cada uma das 4 situações após o jogo da vida (vivos->vivos, vivos->mortos, etc.)\n",
    "            hist_phi_vivos = np.concatenate([np.concatenate([hist_phi_vivos_b,hist_phi_vivos_g]),hist_phi_vivos_r])\n",
    "            hist_phi_mortos = np.concatenate([np.concatenate([hist_phi_mortos_b,hist_phi_mortos_g]),hist_phi_mortos_r])\n",
    "            hist_psi_vivos = np.concatenate([np.concatenate([hist_psi_vivos_b,hist_psi_vivos_g]),hist_psi_vivos_r])\n",
    "            hist_psi_mortos = np.concatenate([np.concatenate([hist_psi_mortos_b,hist_psi_mortos_g]),hist_psi_mortos_r])\n",
    "\n",
    "            #salvando os histogramas\n",
    "            index = os.path.splitext(os.path.basename(imagem_path))[0]\n",
    "            file_path = os.path.join(f\"{histograms_dir}/{classe}\", f\"{index}_phi_vivos_bgr.pkl\")\n",
    "            joblib.dump(hist_phi_vivos, file_path)\n",
    "            file_path = os.path.join(f\"{histograms_dir}/{classe}\", f\"{index}_phi_mortos_bgr.pkl\")\n",
    "            joblib.dump(hist_phi_mortos, file_path)\n",
    "            file_path = os.path.join(f\"{histograms_dir}/{classe}\", f\"{index}_psi_vivos_bgr.pkl\")\n",
    "            joblib.dump(hist_psi_vivos, file_path)\n",
    "            file_path = os.path.join(f\"{histograms_dir}/{classe}\", f\"{index}_psi_mortos_bgr.pkl\")\n",
    "            joblib.dump(hist_psi_mortos, file_path)\n",
    "            progress_bar.update(1)\n",
    "\n",
    "                \n",
    "        progress_bar.close()\n",
    "\n",
    "\n",
    "    print(\"Progresso concluído\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparação de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def carrega_hist_rgb(imagem_path):\n",
    "    #O objetivo dessa função é carregar os histogramas de determinada imagem\n",
    "\n",
    "    #./data/dataset/images/classe/imagem.jpg\n",
    "    imagem_path = imagem_path.split(\"/\")\n",
    "    dataset, classe, imagem = imagem_path[2], imagem_path[4], imagem_path[5]\n",
    "    imagem = imagem.split(\".\")[0]\n",
    "\n",
    "    histogramas_dir = f\"./data/{dataset}/histograms_rgb\"\n",
    "    ##print(imagem_path)\n",
    "    ##print(histogramas_dir)\n",
    "    \n",
    "    try:\n",
    "        hist_phi_vivos = joblib.load(f\"{histogramas_dir}/{classe}/{imagem}_phi_vivos_bgr.pkl\")\n",
    "        hist_phi_mortos = joblib.load(f\"{histogramas_dir}/{classe}/{imagem}_phi_mortos_bgr.pkl\")\n",
    "        hist_psi_vivos = joblib.load(f\"{histogramas_dir}/{classe}/{imagem}_psi_vivos_bgr.pkl\")\n",
    "        hist_psi_mortos = joblib.load(f\"{histogramas_dir}/{classe}/{imagem}_psi_mortos_bgr.pkl\")\n",
    "\n",
    "        return hist_phi_vivos, hist_phi_mortos, hist_psi_vivos, hist_psi_mortos\n",
    "\n",
    "    except:\n",
    "        print(\"Os histogramas não foram encontrados. Verifique os diretórios e se os histogramas foram criados corretamente.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retorna_combinacao_rgb(imagem_path, combinacao):\n",
    "    # O objetivo dessa funcao é criar combinacoes com os histogramas da imagem\n",
    "\n",
    "    \n",
    "    hist_phi_vivos, hist_phi_mortos, hist_psi_vivos, hist_psi_mortos = carrega_hist_rgb(imagem_path)\n",
    "    \n",
    "    switch = {\n",
    "        0: hist_phi_vivos,\n",
    "        1: hist_phi_mortos,\n",
    "        2: hist_psi_vivos,\n",
    "        3: hist_psi_mortos,\n",
    "        4: np.concatenate([hist_phi_vivos, hist_phi_mortos]),   #histogramas phi\n",
    "        5: np.concatenate([hist_psi_vivos, hist_psi_mortos]),   #histogramas psi\n",
    "        6: np.concatenate([hist_phi_vivos, hist_psi_vivos]),    #histogramas vivos\n",
    "        7: np.concatenate([hist_phi_mortos, hist_psi_mortos]),  #histogramas mortos\n",
    "        8: np.concatenate([np.concatenate([hist_phi_vivos, hist_phi_mortos]),\n",
    "                           np.concatenate([hist_psi_vivos, hist_psi_mortos])])\n",
    "    }\n",
    "    \n",
    "    return switch.get(combinacao, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepara os dados para treinar o modelo\n",
    "def retorna_dados_rgb(images_dir): \n",
    "    histogramas_rgb = []\n",
    "    rotulos_rgb = []\n",
    "    print(\"Carregando histogramas\")\n",
    "    for classe in os.listdir(images_dir):\n",
    "        dir_classe = f\"{images_dir}/{classe}\"\n",
    "        classe = f\"{classe}\"\n",
    "        \n",
    "        lista_arquivos = os.listdir(dir_classe)\n",
    "        num_arquivos = len(lista_arquivos)\n",
    "        progress_bar = tqdm(total=num_arquivos, desc= classe)\n",
    "        \n",
    "        for imagem in os.listdir(dir_classe):\n",
    "            imagem_path = f\"{images_dir}/{classe}/{imagem}\"\n",
    "    \n",
    "            #Pode alterar a combinacao \n",
    "            combinacao = retorna_combinacao_rgb(imagem_path, 8)\n",
    "    \n",
    "            #Lista dos histogramas\n",
    "            histogramas_rgb.append(combinacao)\n",
    "            #lista da classe assossiada ao histograma\n",
    "            rotulos_rgb.append(classe)\n",
    "            progress_bar.update(1)\n",
    "\n",
    "        progress_bar.close()\n",
    "    \n",
    "    print(\"Progresso concluído\")\n",
    "    return histogramas_rgb, rotulos_rgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Funções p/ modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian(X_train, y_train, X_test, y_test):\n",
    "    # Gaussian Naive Bayes\n",
    "    gnb = GaussianNB()\n",
    "    model1 = gnb.fit(X_train, y_train)\n",
    "    aux = gnb.predict(X_test)\n",
    "    print(classification_report(y_test, aux, zero_division=1))\n",
    "    #print(confusion_matrix(y_test, aux))\n",
    "\n",
    "def logistic_regr(X_train, y_train, X_test, y_test):\n",
    "    # Logistic Regression\n",
    "    logreg = LogisticRegression(max_iter = 1000)\n",
    "    model2 = logreg.fit(X_train, y_train)\n",
    "    aux = logreg.predict(X_test)\n",
    "    print(classification_report(y_test, aux, zero_division=1))\n",
    "    #print(confusion_matrix(y_test, aux))\n",
    "\n",
    "def decision_tree(X_train, y_train, X_test, y_test):\n",
    "    # Decision Tree\n",
    "    dectree = DecisionTreeClassifier()\n",
    "    model3 = dectree.fit(X_train, y_train)\n",
    "    aux = dectree.predict(X_test)\n",
    "    print(classification_report(y_test, aux, zero_division=1))\n",
    "    #print(confusion_matrix(y_test, aux))\n",
    "\n",
    "def knn(X_train, y_train, X_test, y_test):\n",
    "    # K-Nearest Neighbors\n",
    "    knn = KNeighborsClassifier(n_neighbors = 3)\n",
    "    model4 = knn.fit(X_train, y_train)\n",
    "    aux = knn.predict(X_test)\n",
    "    print(classification_report(y_test, aux, zero_division=1))\n",
    "    #print(confusion_matrix(y_test, aux))\n",
    "\n",
    "def lda(X_train, y_train, X_test, y_test):\n",
    "    # Linear Discriminant Analysis\n",
    "    lda = LinearDiscriminantAnalysis()\n",
    "    model5 = lda.fit(X_train, y_train)\n",
    "    aux = lda.predict(X_test)\n",
    "    print(classification_report(y_test, aux, zero_division=1))\n",
    "    #print(confusion_matrix(y_test, aux))\n",
    "\n",
    "def svm(X_train, y_train, X_test, y_test):\n",
    "    # Support Vector Machine\n",
    "    svm = SVC()\n",
    "    model6 = svm.fit(X_train, y_train)\n",
    "    aux = svm.predict(X_test)\n",
    "    print(classification_report(y_test, aux, zero_division=1))\n",
    "    #print(confusion_matrix(y_test, aux))\n",
    "\n",
    "def random_forest(X_train, y_train, X_test, y_test):\n",
    "    # RandomForest\n",
    "    rf = RandomForestClassifier()\n",
    "    model7 = rf.fit(X_train, y_train)\n",
    "    aux = rf.predict(X_test)\n",
    "    print(classification_report(y_test, aux, zero_division=1))\n",
    "    #print(confusion_matrix(y_test, aux))\n",
    "\n",
    "def neural_net(X_train, y_train, X_test, y_test):\n",
    "    # Neural Net\n",
    "    nnet = MLPClassifier(alpha=1)\n",
    "    model8 = nnet.fit(X_train, y_train)\n",
    "    aux = nnet.predict(X_test)\n",
    "    print(classification_report(y_test, aux, zero_division=1))\n",
    "    #print(confusion_matrix(y_test, aux))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Dataset Corel 1k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EdJ-9FRqJQm1",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Em escala de cinza:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fDQfybOh1Ucj",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Gerando histogramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "anqLr2U22_Fy"
   },
   "outputs": [],
   "source": [
    "#gerador_histogramas(dataset_dir, 128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3o3UPHAE4aWR"
   },
   "source": [
    "### Testando modelos de aprendizado de máquina combinando histogramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T3oLo-dp-fzE"
   },
   "outputs": [],
   "source": [
    "histogramas, rotulos = retorna_dados(images_dir)\n",
    "X = np.array(histogramas)\n",
    "y = np.array(rotulos)\n",
    "\n",
    "#Dividindo treino e teste na respectiva proporção (0.8 / 0.2), stratify = y mantém uma proporção entre os dados de teste de cada classe\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1,stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AtKiSkin-pGz"
   },
   "outputs": [],
   "source": [
    "#Utilizando tecnicas de normalizacao\n",
    "# 1 = MinMaxScaler, 2 = StandardScaler, 3 = MaxAbsScaler, 4 = RobustScaler\n",
    "selectedNormalization = 1\n",
    "\n",
    "if selectedNormalization == 1:\n",
    "    scaler = preprocessing.MinMaxScaler()\n",
    "if selectedNormalization == 2:\n",
    "    scaler = preprocessing.StandardScaler()\n",
    "if selectedNormalization == 3:\n",
    "    scaler = preprocessing.MaxAbsScaler()\n",
    "if selectedNormalization == 4:\n",
    "    scaler = preprocessing.RobustScaler()\n",
    "\n",
    "# Escalando os dados de treinamento\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "# Escalando os dados de teste com os dados de treinamento, visto que os dados de teste podem ser apenas 1 amostra\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q3NP2RnS_x3D",
    "outputId": "a1a2ad1b-07a8-4de8-e360-e3e2f4b22604"
   },
   "outputs": [],
   "source": [
    "gaussian(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oLJhHrGs_0ab",
    "outputId": "6231716e-4054-4ad8-e171-5c7a1aac8240"
   },
   "outputs": [],
   "source": [
    "logistic_regr(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SSUuAjKT_1ez",
    "outputId": "4486eb7a-e510-4c0c-b29c-35edb47a8291"
   },
   "outputs": [],
   "source": [
    "decision_tree(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KUHi1qN4_25j",
    "outputId": "17eec87e-db4b-4a29-c927-2dde5a07812e"
   },
   "outputs": [],
   "source": [
    "knn(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BvzliFey_4CT",
    "outputId": "1c52c97a-2eb0-4560-dc16-a55864279a83"
   },
   "outputs": [],
   "source": [
    "lda(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W2OCnj6a_5l0",
    "outputId": "0d2d4a86-b796-4542-9e90-0661c984bba1"
   },
   "outputs": [],
   "source": [
    "svm(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JfmpE0K8_6iy",
    "outputId": "4fb8783c-936a-451d-8a95-ae34d31478c9"
   },
   "outputs": [],
   "source": [
    "random_forest(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rF_-ULJF_7ZL",
    "outputId": "08c78e30-1c53-4693-957e-609ff1cb1294"
   },
   "outputs": [],
   "source": [
    "neural_net(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Em RGB:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Gerando histogramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gerador_histogramas_RGB(dataset_dir, 128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testando modelos de aprendizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histogramas_rgb, rotulos_rgb = retorna_dados_rgb(images_dir)\n",
    "X = np.array(histogramas_rgb)\n",
    "y = np.array(rotulos_rgb)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Utilizando tecnicas de normalizacao\n",
    "# 1 = MinMaxScaler, 2 = StandardScaler, 3 = MaxAbsScaler, 4 = RobustScaler\n",
    "selectedNormalization = 1\n",
    "\n",
    "if selectedNormalization == 1:\n",
    "    scaler = preprocessing.MinMaxScaler()\n",
    "if selectedNormalization == 2:\n",
    "    scaler = preprocessing.StandardScaler()\n",
    "if selectedNormalization == 3:\n",
    "    scaler = preprocessing.MaxAbsScaler()\n",
    "if selectedNormalization == 4:\n",
    "    scaler = preprocessing.RobustScaler()\n",
    "\n",
    "# Escalando os dados de treinamento\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "# Escalando os dados de teste com os dados de treinamento, visto que os dados de teste podem ser apenas 1 amostra\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regr(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_tree(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_net(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jQy0zeioJZis",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Dataset_1 Caltech256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cVmW9D-CJdwd"
   },
   "source": [
    "## Em RBG:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jzGzBaQfJja-"
   },
   "source": [
    "### Gerando histogramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H53ODYudNj40",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#gerador_histogramas_RGB(dataset_1_dir, 128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M7LdASJ9Jnt-"
   },
   "source": [
    "### Testando modelos de aprendizado "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "histogramas_rgb, rotulos_rgb = retorna_dados_rgb(images_1_dir)\n",
    "X = np.array(histogramas_rgb)\n",
    "y = np.array(rotulos_rgb)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Utilizando tecnicas de normalizacao\n",
    "# 1 = MinMaxScaler, 2 = StandardScaler, 3 = MaxAbsScaler, 4 = RobustScaler\n",
    "selectedNormalization = 1\n",
    "\n",
    "if selectedNormalization == 1:\n",
    "    scaler = preprocessing.MinMaxScaler()\n",
    "if selectedNormalization == 2:\n",
    "    scaler = preprocessing.StandardScaler()\n",
    "if selectedNormalization == 3:\n",
    "    scaler = preprocessing.MaxAbsScaler()\n",
    "if selectedNormalization == 4:\n",
    "    scaler = preprocessing.RobustScaler()\n",
    "\n",
    "# Escalando os dados de treinamento\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "# Escalando os dados de teste com os dados de treinamento, visto que os dados de teste podem ser apenas 1 amostra\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gaussian(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "logistic_regr(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_tree(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_net(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Em escala de cinza:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Gerando histogramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gerador_histogramas(dataset_1_dir, 128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Testando modelos de aprendizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "histogramas, rotulos = retorna_dados(images_1_dir)\n",
    "X = np.array(histogramas)\n",
    "y = np.array(rotulos)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1,stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Utilizando tecnicas de normalizacao\n",
    "# 1 = MinMaxScaler, 2 = StandardScaler, 3 = MaxAbsScaler, 4 = RobustScaler\n",
    "selectedNormalization = 1\n",
    "\n",
    "if selectedNormalization == 1:\n",
    "    scaler = preprocessing.MinMaxScaler()\n",
    "if selectedNormalization == 2:\n",
    "    scaler = preprocessing.StandardScaler()\n",
    "if selectedNormalization == 3:\n",
    "    scaler = preprocessing.MaxAbsScaler()\n",
    "if selectedNormalization == 4:\n",
    "    scaler = preprocessing.RobustScaler()\n",
    "\n",
    "# Escalando os dados de treinamento\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "# Escalando os dados de teste com os dados de treinamento, visto que os dados de teste podem ser apenas 1 amostra\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "logistic_regr(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "decision_tree(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "knn(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lda(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "svm(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "random_forest(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "neural_net(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Dataset_2 CIFAR 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Em escala de cinza:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Gerando histogramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#gerador_histogramas(dataset_2_dir, 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Testando modelos de aprendizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "histogramas, rotulos = retorna_dados(images_2_dir)\n",
    "X = np.array(histogramas)\n",
    "y = np.array(rotulos)\n",
    "\n",
    "#Dividindo treino e teste na respectiva proporção (0.8 / 0.2), stratify = y mantém uma proporção entre os dados de teste de cada classe\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1,stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Utilizando tecnicas de normalizacao\n",
    "# 1 = MinMaxScaler, 2 = StandardScaler, 3 = MaxAbsScaler, 4 = RobustScaler\n",
    "selectedNormalization = 1\n",
    "\n",
    "if selectedNormalization == 1:\n",
    "    scaler = preprocessing.MinMaxScaler()\n",
    "if selectedNormalization == 2:\n",
    "    scaler = preprocessing.StandardScaler()\n",
    "if selectedNormalization == 3:\n",
    "    scaler = preprocessing.MaxAbsScaler()\n",
    "if selectedNormalization == 4:\n",
    "    scaler = preprocessing.RobustScaler()\n",
    "\n",
    "# Escalando os dados de treinamento\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "# Escalando os dados de teste com os dados de treinamento, visto que os dados de teste podem ser apenas 1 amostra\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gaussian(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "logistic_regr(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "decision_tree(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "knn(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lda(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "svm(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "random_forest(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "neural_net(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Em RGB:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Gerando histogramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gerador_histogramas_RGB(dataset_2_dir, 32) #escala em 32 pq o dataset vem em 32x32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Testando modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "histogramas_rgb, rotulos_rgb = retorna_dados_rgb(images_2_dir)\n",
    "X = np.array(histogramas_rgb)\n",
    "y = np.array(rotulos_rgb)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Utilizando tecnicas de normalizacao\n",
    "# 1 = MinMaxScaler, 2 = StandardScaler, 3 = MaxAbsScaler, 4 = RobustScaler\n",
    "selectedNormalization = 1\n",
    "\n",
    "if selectedNormalization == 1:\n",
    "    scaler = preprocessing.MinMaxScaler()\n",
    "if selectedNormalization == 2:\n",
    "    scaler = preprocessing.StandardScaler()\n",
    "if selectedNormalization == 3:\n",
    "    scaler = preprocessing.MaxAbsScaler()\n",
    "if selectedNormalization == 4:\n",
    "    scaler = preprocessing.RobustScaler()\n",
    "\n",
    "# Escalando os dados de treinamento\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "# Escalando os dados de teste com os dados de treinamento, visto que os dados de teste podem ser apenas 1 amostra\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gaussian(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "logistic_regr(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "decision_tree(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "knn(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lda(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "svm(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "random_forest(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "neural_net(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset_3 CIFAR-10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Em escala de cinza:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Gerando histogramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gerador_histogramas(dataset_3_dir, 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testando os modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "histogramas, rotulos = retorna_dados(images_3_dir)\n",
    "X = np.array(histogramas)\n",
    "y = np.array(rotulos)\n",
    "\n",
    "#Dividindo treino e teste na respectiva proporção (0.8 / 0.2), stratify = y mantém uma proporção entre os dados de teste de cada classe\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1,stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Utilizando tecnicas de normalizacao\n",
    "# 1 = MinMaxScaler, 2 = StandardScaler, 3 = MaxAbsScaler, 4 = RobustScaler\n",
    "selectedNormalization = 1\n",
    "\n",
    "if selectedNormalization == 1:\n",
    "    scaler = preprocessing.MinMaxScaler()\n",
    "if selectedNormalization == 2:\n",
    "    scaler = preprocessing.StandardScaler()\n",
    "if selectedNormalization == 3:\n",
    "    scaler = preprocessing.MaxAbsScaler()\n",
    "if selectedNormalization == 4:\n",
    "    scaler = preprocessing.RobustScaler()\n",
    "\n",
    "# Escalando os dados de treinamento\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "# Escalando os dados de teste com os dados de treinamento, visto que os dados de teste podem ser apenas 1 amostra\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regr(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_tree(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_net(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Em RGB:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gerando histogramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gerando histogramas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "airplane: 100%|█████████████████████████████████████████████████████████████| 6000/6000 [30:21<00:00,  3.29it/s]\n",
      "automobile: 100%|███████████████████████████████████████████████████████████| 6000/6000 [17:03<00:00,  5.86it/s]\n",
      "bird: 100%|█████████████████████████████████████████████████████████████████| 6000/6000 [16:59<00:00,  5.89it/s]\n",
      "cat: 100%|██████████████████████████████████████████████████████████████████| 6000/6000 [16:55<00:00,  5.91it/s]\n",
      "deer: 100%|█████████████████████████████████████████████████████████████████| 6000/6000 [15:58<00:00,  6.26it/s]\n",
      "dog: 100%|██████████████████████████████████████████████████████████████████| 6000/6000 [15:23<00:00,  6.50it/s]\n",
      "frog: 100%|█████████████████████████████████████████████████████████████████| 6000/6000 [16:30<00:00,  6.06it/s]\n",
      "horse: 100%|████████████████████████████████████████████████████████████████| 6000/6000 [17:30<00:00,  5.71it/s]\n",
      "ship: 100%|█████████████████████████████████████████████████████████████████| 6000/6000 [15:47<00:00,  6.33it/s]\n",
      "truck: 100%|████████████████████████████████████████████████████████████████| 6000/6000 [16:11<00:00,  6.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progresso concluído\n"
     ]
    }
   ],
   "source": [
    "gerador_histogramas_RGB(dataset_3_dir, 32) #escala em 32 pq o dataset vem em 32x32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testando modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carregando histogramas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "airplane: 100%|████████████████████████████████████████████████████████████| 6000/6000 [00:14<00:00, 418.99it/s]\n",
      "automobile: 100%|██████████████████████████████████████████████████████████| 6000/6000 [00:13<00:00, 455.42it/s]\n",
      "bird: 100%|████████████████████████████████████████████████████████████████| 6000/6000 [00:13<00:00, 439.75it/s]\n",
      "cat: 100%|█████████████████████████████████████████████████████████████████| 6000/6000 [00:13<00:00, 452.22it/s]\n",
      "deer: 100%|████████████████████████████████████████████████████████████████| 6000/6000 [00:13<00:00, 447.96it/s]\n",
      "dog: 100%|█████████████████████████████████████████████████████████████████| 6000/6000 [00:13<00:00, 448.63it/s]\n",
      "frog: 100%|████████████████████████████████████████████████████████████████| 6000/6000 [00:13<00:00, 430.16it/s]\n",
      "horse: 100%|███████████████████████████████████████████████████████████████| 6000/6000 [00:12<00:00, 477.25it/s]\n",
      "ship: 100%|████████████████████████████████████████████████████████████████| 6000/6000 [00:12<00:00, 462.14it/s]\n",
      "truck: 100%|███████████████████████████████████████████████████████████████| 6000/6000 [00:12<00:00, 491.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progresso concluído\n"
     ]
    }
   ],
   "source": [
    "histogramas_rgb, rotulos_rgb = retorna_dados_rgb(images_3_dir)\n",
    "X = np.array(histogramas_rgb)\n",
    "y = np.array(rotulos_rgb)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Utilizando tecnicas de normalizacao\n",
    "# 1 = MinMaxScaler, 2 = StandardScaler, 3 = MaxAbsScaler, 4 = RobustScaler\n",
    "selectedNormalization = 1\n",
    "\n",
    "if selectedNormalization == 1:\n",
    "    scaler = preprocessing.MinMaxScaler()\n",
    "if selectedNormalization == 2:\n",
    "    scaler = preprocessing.StandardScaler()\n",
    "if selectedNormalization == 3:\n",
    "    scaler = preprocessing.MaxAbsScaler()\n",
    "if selectedNormalization == 4:\n",
    "    scaler = preprocessing.RobustScaler()\n",
    "\n",
    "# Escalando os dados de treinamento\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "# Escalando os dados de teste com os dados de treinamento, visto que os dados de teste podem ser apenas 1 amostra\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    airplane       0.42      0.32      0.36      1200\n",
      "  automobile       0.23      0.05      0.09      1200\n",
      "        bird       0.30      0.12      0.17      1200\n",
      "         cat       0.23      0.08      0.12      1200\n",
      "        deer       0.23      0.35      0.28      1200\n",
      "         dog       0.22      0.12      0.15      1200\n",
      "        frog       0.20      0.55      0.30      1200\n",
      "       horse       0.24      0.04      0.07      1200\n",
      "        ship       0.28      0.24      0.26      1200\n",
      "       truck       0.22      0.52      0.31      1200\n",
      "\n",
      "    accuracy                           0.24     12000\n",
      "   macro avg       0.26      0.24      0.21     12000\n",
      "weighted avg       0.26      0.24      0.21     12000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gaussian(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    airplane       0.40      0.40      0.40      1200\n",
      "  automobile       0.28      0.30      0.29      1200\n",
      "        bird       0.29      0.21      0.24      1200\n",
      "         cat       0.23      0.17      0.19      1200\n",
      "        deer       0.28      0.35      0.31      1200\n",
      "         dog       0.25      0.24      0.25      1200\n",
      "        frog       0.34      0.47      0.40      1200\n",
      "       horse       0.28      0.18      0.22      1200\n",
      "        ship       0.36      0.40      0.38      1200\n",
      "       truck       0.31      0.37      0.34      1200\n",
      "\n",
      "    accuracy                           0.31     12000\n",
      "   macro avg       0.30      0.31      0.30     12000\n",
      "weighted avg       0.30      0.31      0.30     12000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logistic_regr(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    airplane       0.29      0.27      0.28      1200\n",
      "  automobile       0.20      0.21      0.21      1200\n",
      "        bird       0.17      0.17      0.17      1200\n",
      "         cat       0.14      0.14      0.14      1200\n",
      "        deer       0.21      0.21      0.21      1200\n",
      "         dog       0.14      0.14      0.14      1200\n",
      "        frog       0.26      0.27      0.27      1200\n",
      "       horse       0.15      0.15      0.15      1200\n",
      "        ship       0.24      0.23      0.23      1200\n",
      "       truck       0.22      0.23      0.22      1200\n",
      "\n",
      "    accuracy                           0.20     12000\n",
      "   macro avg       0.20      0.20      0.20     12000\n",
      "weighted avg       0.20      0.20      0.20     12000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "decision_tree(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    airplane       0.34      0.40      0.37      1200\n",
      "  automobile       0.22      0.41      0.29      1200\n",
      "        bird       0.17      0.25      0.21      1200\n",
      "         cat       0.15      0.22      0.18      1200\n",
      "        deer       0.24      0.23      0.24      1200\n",
      "         dog       0.17      0.10      0.12      1200\n",
      "        frog       0.28      0.39      0.32      1200\n",
      "       horse       0.26      0.08      0.12      1200\n",
      "        ship       0.46      0.16      0.24      1200\n",
      "       truck       0.30      0.12      0.17      1200\n",
      "\n",
      "    accuracy                           0.24     12000\n",
      "   macro avg       0.26      0.24      0.23     12000\n",
      "weighted avg       0.26      0.24      0.23     12000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "knn(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    airplane       0.40      0.35      0.37      1200\n",
      "  automobile       0.26      0.29      0.27      1200\n",
      "        bird       0.26      0.18      0.21      1200\n",
      "         cat       0.19      0.17      0.18      1200\n",
      "        deer       0.28      0.35      0.31      1200\n",
      "         dog       0.22      0.23      0.23      1200\n",
      "        frog       0.32      0.39      0.35      1200\n",
      "       horse       0.19      0.15      0.17      1200\n",
      "        ship       0.32      0.32      0.32      1200\n",
      "       truck       0.28      0.31      0.30      1200\n",
      "\n",
      "    accuracy                           0.28     12000\n",
      "   macro avg       0.27      0.28      0.27     12000\n",
      "weighted avg       0.27      0.28      0.27     12000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lda(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_net(X_train, y_train, X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "fDQfybOh1Ucj",
    "3o3UPHAE4aWR"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

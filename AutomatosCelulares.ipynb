{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DL-xS3wo1LtM",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Importanto Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "MINx5gXN1Pfc"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Corel 1k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "meNzZUIM2FYi",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Definindo os diretórios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "iiQLxSFC2K-x"
   },
   "outputs": [],
   "source": [
    "data_dir = f\"./data\"\n",
    "\n",
    "#corel-1k\n",
    "dataset_dir = f\"{data_dir}/dataset\"\n",
    "images_dir = f\"{dataset_dir}/images\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EdJ-9FRqJQm1",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Em escala de cinza:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fDQfybOh1Ucj",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Gerando histogramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "20US-kPt1ewz"
   },
   "outputs": [],
   "source": [
    "def obter_vizinhos(matriz_de_intensidade, linha, coluna):\n",
    "    # O objetivo dessa função é retornar uma lista contendo a intensidade dos 9 vizinhos do pixel observado\n",
    "\n",
    "    #Esse método de partição da matriz foi escolhido para manter a analise dos pixels vizinhos\n",
    "    #dentro dos limites da matriz de intensidade, evitando assim valores negativos ou fora do shape.\n",
    "    vizinhos = matriz_de_intensidade[max(0, linha-1):min(matriz_de_intensidade.shape[0], linha+2),\n",
    "                             max(0, coluna-1):min(matriz_de_intensidade.shape[1], coluna+2)]\n",
    "\n",
    "    #Transforma a matriz particionada em lista e remove o pixel do centro\n",
    "    lista_de_vizinhos = vizinhos.flatten().tolist()\n",
    "    lista_de_vizinhos.remove(matriz_de_intensidade[linha, coluna])\n",
    "\n",
    "    return lista_de_vizinhos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eB0hcmul1mSj"
   },
   "outputs": [],
   "source": [
    "def aplica_regras(lista_de_vizinhos, intensidade_pixel_central, estado_pixel_central):\n",
    "    #O objetivo dessa função é aplicar as regras do jogo da vida de Conway no pixel observado e retornar seu estado.\n",
    "    # 1 = vivo, 0 = morto\n",
    "\n",
    "    #Conta o número de vizinhos vivos com a mesma intensidade que o pixel central\n",
    "    vizinhos_iguais = lista_de_vizinhos.count(intensidade_pixel_central)\n",
    "\n",
    "    #Regra 1: A célula viva com dois ou três vizinhos vivos sobrevive\n",
    "    if estado_pixel_central == 1 and (vizinhos_iguais == 2 or vizinhos_iguais == 3):\n",
    "        return 1\n",
    "\n",
    "    #Regra 2: A célula viva com menos de dois vizinhos vivos morre (subpopulação)\n",
    "    elif estado_pixel_central == 1 and vizinhos_iguais < 2:\n",
    "        return 0\n",
    "\n",
    "    #Regra 3: A célula viva com mais de três vizinhos vivos morre (superpopulação)\n",
    "    elif estado_pixel_central == 1 and vizinhos_iguais > 3:\n",
    "        return 0\n",
    "\n",
    "    #Regra 4: A célula morta com exatamente três vizinhos vivos se torna viva (resurreição)\n",
    "    elif estado_pixel_central == 0 and vizinhos_iguais == 3:\n",
    "        return 1\n",
    "\n",
    "    #Pra todos os outros casos, a célula permanece no mesmo estado\n",
    "    else:\n",
    "        return estado_pixel_central"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EtJZBP-_1q0C"
   },
   "outputs": [],
   "source": [
    "def percorre_imagem_aplicando_regras(matriz_de_estados, matriz_de_intensidade):\n",
    "    # O objetivo dessa função é percorrer a imagem, chamar a função para obter os vizinhos e aplicar as regras\n",
    "\n",
    "    linhas, colunas = matriz_de_intensidade.shape\n",
    "    for linha in range(linhas):\n",
    "        for coluna in range(colunas):\n",
    "            #Obtem os vizinhos do pixel atual\n",
    "            lista_de_vizinhos = obter_vizinhos(matriz_de_intensidade, linha, coluna)\n",
    "            #Aplica as regras do jogo da vida no pixel atual (atualiza a matriz de estado inicial)\n",
    "            matriz_de_estados[linha, coluna] = aplica_regras(lista_de_vizinhos, matriz_de_intensidade[linha, coluna], matriz_de_estados[linha, coluna])\n",
    "\n",
    "    return matriz_de_estados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dB9wyi2N1ylS"
   },
   "outputs": [],
   "source": [
    "def gera_histogramas(imagem_cinza):\n",
    "    #O objetivo dessa função é criar as matriz de intensidade e as de estado inicial para cada imagem\n",
    "    #Após aplicar as regras cria os histogramas\n",
    "\n",
    "    #Transforma a imagem em uma matriz de intensidade\n",
    "    matriz_de_intensidade = np.array(imagem_cinza)\n",
    "\n",
    "    #Cria as matrizes de estados iniciais\n",
    "    matriz_de_estados_phi = np.ones(matriz_de_intensidade.shape, dtype=int) #todos vivos\n",
    "    matriz_de_estados_psi = np.zeros(matriz_de_intensidade.shape, dtype=int) #todos mortos\n",
    "\n",
    "    #Aplica as regras do jogo da vida e atualiza as matrizes de estado inicial\n",
    "    matriz_de_estados_phi = percorre_imagem_aplicando_regras(matriz_de_estados_phi, matriz_de_intensidade)\n",
    "    matriz_de_estados_psi = percorre_imagem_aplicando_regras(matriz_de_estados_psi, matriz_de_intensidade)\n",
    "\n",
    "    #As matrizes são convertidas em listas\n",
    "    #Phi -> estado inicial = vivo\n",
    "    phi_vivos = matriz_de_intensidade.flatten()[matriz_de_estados_phi.flatten() == 1] #se manteram vivos\n",
    "    phi_mortos = matriz_de_intensidade.flatten()[matriz_de_estados_phi.flatten() == 0] #morreram\n",
    "\n",
    "    #Psi -> estado inicial = morto\n",
    "    psi_vivos = matriz_de_intensidade.flatten()[matriz_de_estados_psi.flatten() == 1] #ressuscitaram\n",
    "    psi_mortos = matriz_de_intensidade.flatten()[matriz_de_estados_psi.flatten() == 0] #se manteram mortos\n",
    "\n",
    "    #Cria os histogramas\n",
    "    hist_phi_vivos, _ = np.histogram(phi_vivos, bins=256, range=(0, 256))\n",
    "    hist_phi_mortos, _ = np.histogram(phi_mortos, bins=256, range=(0, 256))\n",
    "    hist_psi_vivos, _ = np.histogram(psi_vivos, bins=256, range=(0, 256))\n",
    "    hist_psi_mortos, _ = np.histogram(psi_mortos, bins=256, range=(0, 256))\n",
    "\n",
    "    return hist_phi_vivos, hist_phi_mortos, hist_psi_vivos, hist_psi_mortos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bov-XA3a17a6"
   },
   "outputs": [],
   "source": [
    "def gerador_histogramas(dataset_dir):\n",
    "    #Funcao principal: faz a chamada das funções acima e salva os histogramas\n",
    "\n",
    "    images_dir = f\"{dataset_dir}/images\"\n",
    "    histograms_dir = f\"{dataset_dir}/histograms_grayscale\"\n",
    "\n",
    "    # Cria as pastas para salvar os histogramas mantendo o padrão das classes do dataset\n",
    "    os.makedirs(histograms_dir, exist_ok=True)\n",
    "\n",
    "    classes = [conteudo_item for conteudo_item in os.listdir(images_dir) if os.path.isdir(os.path.join(images_dir, conteudo_item))]\n",
    "    for classe in classes:\n",
    "        dir_pastas = os.path.join(histograms_dir, classe)\n",
    "        os.makedirs(dir_pastas, exist_ok=True)\n",
    "\n",
    "    i = 0\n",
    "    #Esse loop pega a imagem, gera seus histogramas e salva com base no nome da classe e da imagem\n",
    "    for classe in os.listdir(images_dir):\n",
    "        dir_classe = f\"{images_dir}/{classe}\"\n",
    "\n",
    "        for imagem in os.listdir(dir_classe):\n",
    "            imagem_path = f\"{images_dir}/{classe}/{imagem}\"\n",
    "\n",
    "            imagem_cinza = cv2.imread(imagem_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "            imagem_cinza = cv2.resize(imagem_cinza, (128,128))\n",
    "            #Dimensao original = 384 * 256 = 98304\n",
    "            #Resize = 128 * 128 = 16384\n",
    "            #Redução = (16384-98304)/98304*100 = 83,33%\n",
    "\n",
    "            hist_phi_vivos, hist_phi_mortos, hist_psi_vivos, hist_psi_mortos = gera_histogramas(imagem_cinza)\n",
    "\n",
    "            index = imagem.split(\".\")[0]\n",
    "            file_path = os.path.join(f\"{histograms_dir}/{classe}\", f\"{index}_phi_vivos.pkl\")\n",
    "            joblib.dump(hist_phi_vivos, file_path)\n",
    "            file_path = os.path.join(f\"{histograms_dir}/{classe}\", f\"{index}_phi_mortos.pkl\")\n",
    "            joblib.dump(hist_phi_mortos, file_path)\n",
    "            file_path = os.path.join(f\"{histograms_dir}/{classe}\", f\"{index}_psi_vivos.pkl\")\n",
    "            joblib.dump(hist_psi_vivos, file_path)\n",
    "            file_path = os.path.join(f\"{histograms_dir}/{classe}\", f\"{index}_psi_mortos.pkl\")\n",
    "            joblib.dump(hist_psi_mortos, file_path)\n",
    "\n",
    "            i += 1\n",
    "            if i % 10 == 0:\n",
    "              print(\"Progresso = %0.1f por cento\" % (i / 1000 * 100))\n",
    "\n",
    "    print(\"Progresso concluído\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "anqLr2U22_Fy"
   },
   "outputs": [],
   "source": [
    "#gerador_histogramas(dataset_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3o3UPHAE4aWR",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Testando modelos de aprendizado de máquina combinando histogramas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Carregando histogramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X1-hDPR441YB"
   },
   "outputs": [],
   "source": [
    "def carrega_hist(imagem_path):\n",
    "    #O objetivo dessa função é carregar os histogramas de determinada imagem\n",
    "\n",
    "    #data\\dataset\\images\\beaches\\100.jpg\n",
    "    imagem_path = imagem_path.split(\"/\")\n",
    "    classe, imagem = imagem_path[4], imagem_path[5]\n",
    "    imagem = imagem.split(\".\")[0]\n",
    "\n",
    "    \n",
    "    histogramas_dir = f\"{dataset_dir}/histograms_grayscale\"\n",
    "    try:\n",
    "        hist_phi_vivos = joblib.load(f\"{histogramas_dir}/{classe}/{imagem}_phi_vivos.pkl\")\n",
    "        hist_phi_mortos = joblib.load(f\"{histogramas_dir}/{classe}/{imagem}_phi_mortos.pkl\")\n",
    "        hist_psi_vivos = joblib.load(f\"{histogramas_dir}/{classe}/{imagem}_psi_vivos.pkl\")\n",
    "        hist_psi_mortos = joblib.load(f\"{histogramas_dir}/{classe}/{imagem}_psi_mortos.pkl\")\n",
    "\n",
    "        return hist_phi_vivos, hist_phi_mortos, hist_psi_vivos, hist_psi_mortos\n",
    "\n",
    "    except:\n",
    "        print(\"Os histogramas não foram encontrados. Verifique os diretórios e se os histogramas foram criados corretamente.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5Czi-6on54T3"
   },
   "outputs": [],
   "source": [
    "def retorna_combinacao(imagem_path, combinacao):\n",
    "    # O objetivo dessa funcao é criar combinacoes com os histogramas da imagem\n",
    "\n",
    "    hist_phi_vivos, hist_phi_mortos, hist_psi_vivos, hist_psi_mortos = carrega_hist(imagem_path)\n",
    "    if combinacao == 0:\n",
    "        return hist_phi_vivos\n",
    "    elif combinacao == 1:\n",
    "        return hist_phi_mortos\n",
    "    elif combinacao == 2:\n",
    "        return hist_psi_vivos\n",
    "    elif combinacao == 3:\n",
    "        return hist_psi_mortos\n",
    "    elif combinacao == 4:\n",
    "        return np.concatenate([hist_phi_vivos, hist_phi_mortos]) #histogramas phi\n",
    "    elif combinacao == 5:\n",
    "        return np.concatenate([hist_psi_vivos, hist_psi_mortos]) #histogramas psi\n",
    "    elif combinacao == 6:\n",
    "        return np.concatenate([hist_phi_vivos, hist_psi_vivos]) #histogramas vivos\n",
    "    elif combinacao == 7:\n",
    "        return np.concatenate([hist_phi_mortos, hist_psi_mortos]) #histogramas mortos\n",
    "    elif combinacao == 8:\n",
    "        return np.concatenate([np.concatenate([hist_phi_vivos, hist_phi_mortos]),\n",
    "                               np.concatenate([hist_psi_vivos, hist_psi_mortos])])\n",
    "        #histogramas phi e psi combinados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "YaknpHZC6JAK",
    "outputId": "c13ce5b2-0179-43fc-cf8e-51dd3fc9297a"
   },
   "outputs": [],
   "source": [
    "'''0 -> \"hist_phi_vivos\", 1 -> \"hist_phi_mortos\", 2 -> \"hist_psi_vivos\", 3 -> \"hist_psi_mortos\",\n",
    "4 -> \"hist_phi_combined\", 5 -> \"hist_psi_combined\", 6 -> \"hist_vivos_combined\",\n",
    "7 -> \"hist_mortos_combined\", 8 -> \"all_combined\" '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Preparando dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8QDwf8Vi8pqX",
    "outputId": "9360cd68-3f16-440a-8771-1933d0e00b81"
   },
   "outputs": [],
   "source": [
    "# Prepara os dados para treinar o modelo\n",
    "\n",
    "histogramas = []\n",
    "rotulos = []\n",
    "\n",
    "for classe in os.listdir(images_dir):\n",
    "    dir_classe = f\"{images_dir}/{classe}\"\n",
    "    for imagem in os.listdir(dir_classe):\n",
    "        imagem_path = f\"{images_dir}/{classe}/{imagem}\"\n",
    "\n",
    "        #Pode alterar a combinacao\n",
    "        combinacao = retorna_combinacao(imagem_path, 8)\n",
    "\n",
    "        #Lista dos histogramas\n",
    "        histogramas.append(combinacao)\n",
    "        #lista da classe assossiada ao histograma\n",
    "        rotulos.append(classe)\n",
    "\n",
    "print(\"Progresso concluído\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T3oLo-dp-fzE"
   },
   "outputs": [],
   "source": [
    "X = np.array(histogramas)\n",
    "y = np.array(rotulos)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1,stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AtKiSkin-pGz"
   },
   "outputs": [],
   "source": [
    "#Utilizando tecnicas de normalizacao\n",
    "# 1 = MinMaxScaler, 2 = StandardScaler, 3 = MaxAbsScaler, 4 = RobustScaler\n",
    "selectedNormalization = 1\n",
    "\n",
    "if selectedNormalization == 1:\n",
    "    scaler = preprocessing.MinMaxScaler()\n",
    "if selectedNormalization == 2:\n",
    "    scaler = preprocessing.StandardScaler()\n",
    "if selectedNormalization == 3:\n",
    "    scaler = preprocessing.MaxAbsScaler()\n",
    "if selectedNormalization == 4:\n",
    "    scaler = preprocessing.RobustScaler()\n",
    "\n",
    "# Escalando os dados de treinamento\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "# Escalando os dados de teste com os dados de treinamento, visto que os dados de teste podem ser apenas 1 amostra\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Testando modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q3NP2RnS_x3D",
    "outputId": "a1a2ad1b-07a8-4de8-e360-e3e2f4b22604"
   },
   "outputs": [],
   "source": [
    "# Gaussian Naive Bayes\n",
    "gnb = GaussianNB()\n",
    "model1 = gnb.fit(X_train, y_train)\n",
    "aux = gnb.predict(X_test)\n",
    "print(classification_report(y_test, aux))\n",
    "print(confusion_matrix(y_test, aux))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oLJhHrGs_0ab",
    "outputId": "6231716e-4054-4ad8-e171-5c7a1aac8240"
   },
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "logreg = LogisticRegression()\n",
    "model2 = logreg.fit(X_train, y_train)\n",
    "aux = logreg.predict(X_test)\n",
    "print(classification_report(y_test, aux))\n",
    "print(confusion_matrix(y_test, aux))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SSUuAjKT_1ez",
    "outputId": "4486eb7a-e510-4c0c-b29c-35edb47a8291"
   },
   "outputs": [],
   "source": [
    "# Decision Tree\n",
    "dectree = DecisionTreeClassifier()\n",
    "model3 = dectree.fit(X_train, y_train)\n",
    "aux = dectree.predict(X_test)\n",
    "print(classification_report(y_test, aux))\n",
    "print(confusion_matrix(y_test, aux))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KUHi1qN4_25j",
    "outputId": "17eec87e-db4b-4a29-c927-2dde5a07812e"
   },
   "outputs": [],
   "source": [
    "# K-Nearest Neighbors\n",
    "knn = KNeighborsClassifier(n_neighbors = 3)\n",
    "model4 = knn.fit(X_train, y_train)\n",
    "aux = knn.predict(X_test)\n",
    "print(classification_report(y_test, aux))\n",
    "print(confusion_matrix(y_test, aux))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BvzliFey_4CT",
    "outputId": "1c52c97a-2eb0-4560-dc16-a55864279a83"
   },
   "outputs": [],
   "source": [
    "# Linear Discriminant Analysis\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "model5 = lda.fit(X_train, y_train)\n",
    "aux = lda.predict(X_test)\n",
    "print(classification_report(y_test, aux))\n",
    "print(confusion_matrix(y_test, aux))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W2OCnj6a_5l0",
    "outputId": "0d2d4a86-b796-4542-9e90-0661c984bba1"
   },
   "outputs": [],
   "source": [
    "# Support Vector Machine\n",
    "svm = SVC()\n",
    "model6 = svm.fit(X_train, y_train)\n",
    "aux = svm.predict(X_test)\n",
    "print(classification_report(y_test, aux))\n",
    "print(confusion_matrix(y_test, aux))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JfmpE0K8_6iy",
    "outputId": "4fb8783c-936a-451d-8a95-ae34d31478c9"
   },
   "outputs": [],
   "source": [
    "# RandomForest\n",
    "rf = RandomForestClassifier()\n",
    "model7 = rf.fit(X_train, y_train)\n",
    "aux = rf.predict(X_test)\n",
    "print(classification_report(y_test, aux))\n",
    "print(confusion_matrix(y_test, aux))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rF_-ULJF_7ZL",
    "outputId": "08c78e30-1c53-4693-957e-609ff1cb1294"
   },
   "outputs": [],
   "source": [
    "# Neural Net\n",
    "nnet = MLPClassifier(alpha=1)\n",
    "model8 = nnet.fit(X_train, y_train)\n",
    "aux = nnet.predict(X_test)\n",
    "print(classification_report(y_test, aux))\n",
    "print(confusion_matrix(y_test, aux))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Em RGB:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Gerando histogramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obter_vizinhos_rgb(matriz_de_intensidade, linha, coluna):\n",
    "    # O objetivo dessa função é retornar uma lista contendo a intensidade dos 9 vizinhos do pixel observado\n",
    "\n",
    "    #Esse método de partição da matriz foi escolhido para manter a analise dos pixels vizinhos\n",
    "    #dentro dos limites da matriz de intensidade, evitando assim valores negativos ou fora do shape.\n",
    "    vizinhos = matriz_de_intensidade[max(0, linha-1):min(matriz_de_intensidade.shape[0], linha+2),\n",
    "                             max(0, coluna-1):min(matriz_de_intensidade.shape[1], coluna+2)]\n",
    "\n",
    "    #Transforma a matriz particionada em lista e remove o pixel do centro\n",
    "    lista_de_vizinhos = vizinhos.flatten().tolist()\n",
    "    lista_de_vizinhos.remove(matriz_de_intensidade[linha, coluna])\n",
    "\n",
    "    return lista_de_vizinhos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aplica_regras_rgb(lista_de_vizinhos, intensidade_pixel_central, estado_pixel_central):\n",
    "    #O objetivo dessa função é aplicar as regras do jogo da vida de Conway no pixel observado e retornar seu estado.\n",
    "    # 1 = vivo, 0 = morto\n",
    "\n",
    "    #Conta o número de vizinhos vivos com a mesma intensidade que o pixel central\n",
    "    vizinhos_iguais = lista_de_vizinhos.count(intensidade_pixel_central)\n",
    "\n",
    "    #Regra 1: A célula viva com dois ou três vizinhos vivos sobrevive\n",
    "    if estado_pixel_central == 1 and (vizinhos_iguais == 2 or vizinhos_iguais == 3):\n",
    "        return 1\n",
    "\n",
    "    #Regra 2: A célula viva com menos de dois vizinhos vivos morre (subpopulação)\n",
    "    elif estado_pixel_central == 1 and vizinhos_iguais < 2:\n",
    "        return 0\n",
    "\n",
    "    #Regra 3: A célula viva com mais de três vizinhos vivos morre (superpopulação)\n",
    "    elif estado_pixel_central == 1 and vizinhos_iguais > 3:\n",
    "        return 0\n",
    "\n",
    "    #Regra 4: A célula morta com exatamente três vizinhos vivos se torna viva (resurreição)\n",
    "    elif estado_pixel_central == 0 and vizinhos_iguais == 3:\n",
    "        return 1\n",
    "\n",
    "    #Pra todos os outros casos, a célula permanece no mesmo estado\n",
    "    else:\n",
    "        return estado_pixel_central"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def percorre_imagem_aplicando_regras_rgb(matriz_de_estados, matriz_de_intensidade):\n",
    "    # O objetivo dessa função é percorrer a imagem, chamar a função para obter os vizinhos e aplicar as regras\n",
    "\n",
    "    linhas, colunas = matriz_de_intensidade.shape\n",
    "    for linha in range(linhas):\n",
    "        for coluna in range(colunas):\n",
    "            #Obtem os vizinhos do pixel atual\n",
    "            lista_de_vizinhos = obter_vizinhos_rgb(matriz_de_intensidade, linha, coluna)\n",
    "            #Aplica as regras do jogo da vida no pixel atual (atualiza a matriz de estado inicial)\n",
    "            matriz_de_estados[linha, coluna] = aplica_regras_rgb(lista_de_vizinhos, matriz_de_intensidade[linha, coluna], matriz_de_estados[linha, coluna])\n",
    "\n",
    "    return matriz_de_estados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gera_histogramas_rgb(imagem):\n",
    "    #O objetivo dessa função é criar as matriz de intensidade e as de estado inicial para cada imagem\n",
    "    #Após aplicar as regras cria os histogramas\n",
    "\n",
    "    #Transforma a imagem em uma matriz de intensidade\n",
    "    matriz_de_intensidade = np.array(imagem)\n",
    "\n",
    "    #Cria as matrizes de estados iniciais\n",
    "    matriz_de_estados_phi = np.ones(matriz_de_intensidade.shape, dtype=int) #todos vivos\n",
    "    matriz_de_estados_psi = np.zeros(matriz_de_intensidade.shape, dtype=int) #todos mortos\n",
    "\n",
    "    #Aplica as regras do jogo da vida e atualiza as matrizes de estado inicial\n",
    "    matriz_de_estados_phi = percorre_imagem_aplicando_regras_rgb(matriz_de_estados_phi, matriz_de_intensidade)\n",
    "    matriz_de_estados_psi = percorre_imagem_aplicando_regras_rgb(matriz_de_estados_psi, matriz_de_intensidade)\n",
    "\n",
    "    #As matrizes são convertidas em listas\n",
    "    #Phi -> estado inicial = vivo\n",
    "    phi_vivos = matriz_de_intensidade.flatten()[matriz_de_estados_phi.flatten() == 1] #se manteram vivos\n",
    "    phi_mortos = matriz_de_intensidade.flatten()[matriz_de_estados_phi.flatten() == 0] #morreram\n",
    "\n",
    "    #Psi -> estado inicial = morto\n",
    "    psi_vivos = matriz_de_intensidade.flatten()[matriz_de_estados_psi.flatten() == 1] #ressuscitaram\n",
    "    psi_mortos = matriz_de_intensidade.flatten()[matriz_de_estados_psi.flatten() == 0] #se manteram mortos\n",
    "\n",
    "    #Cria os histogramas\n",
    "    hist_phi_vivos, _ = np.histogram(phi_vivos, bins=256, range=(0, 256))\n",
    "    hist_phi_mortos, _ = np.histogram(phi_mortos, bins=256, range=(0, 256))\n",
    "    hist_psi_vivos, _ = np.histogram(psi_vivos, bins=256, range=(0, 256))\n",
    "    hist_psi_mortos, _ = np.histogram(psi_mortos, bins=256, range=(0, 256))\n",
    "\n",
    "    return hist_phi_vivos, hist_phi_mortos, hist_psi_vivos, hist_psi_mortos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gerador_histogramas_RGB(dataset_dir):\n",
    "    images_dir = f\"{dataset_dir}/images\"\n",
    "    histograms_dir = f\"{dataset_dir}/histograms_rgb\"\n",
    "    os.makedirs(histograms_dir, exist_ok=True)\n",
    "\n",
    "    classes = [d for d in os.listdir(images_dir) if os.path.isdir(os.path.join(images_dir, d))]\n",
    "    for classe in classes:\n",
    "        os.makedirs(os.path.join(histograms_dir, classe), exist_ok=True)\n",
    "\n",
    "    for classe in classes:\n",
    "        print(classe)\n",
    "        dir_classe = os.path.join(images_dir, classe)\n",
    "        for imagem in os.listdir(dir_classe):\n",
    "            try:\n",
    "                imagem_path = os.path.join(dir_classe, imagem)\n",
    "                imagem_rgb = cv2.imread(imagem_path, cv2.IMREAD_COLOR)\n",
    "                imagem_rgb = cv2.resize(imagem_rgb, (128, 128))  # Resize para 128x128\n",
    "                \n",
    "                hist_phi_vivos_b, hist_phi_mortos_b, hist_psi_vivos_b, hist_psi_mortos_b = gera_histogramas_rgb(imagem_rgb[:, :, 0])\n",
    "                hist_phi_vivos_g, hist_phi_mortos_g, hist_psi_vivos_g, hist_psi_mortos_g = gera_histogramas_rgb(imagem_rgb[:, :, 1])\n",
    "                hist_phi_vivos_r, hist_phi_mortos_r, hist_psi_vivos_r, hist_psi_mortos_r = gera_histogramas_rgb(imagem_rgb[:, :, 2])\n",
    "                \n",
    "                hist_phi_vivos = np.concatenate([np.concatenate([hist_phi_vivos_b,hist_phi_vivos_g]),hist_phi_vivos_r])\n",
    "                hist_phi_mortos = np.concatenate([np.concatenate([hist_phi_mortos_b,hist_phi_mortos_g]),hist_phi_mortos_r])\n",
    "                hist_psi_vivos = np.concatenate([np.concatenate([hist_psi_vivos_b,hist_psi_vivos_g]),hist_psi_vivos_r])\n",
    "                hist_psi_mortos = np.concatenate([np.concatenate([hist_psi_mortos_b,hist_psi_mortos_g]),hist_psi_mortos_r])\n",
    "                    \n",
    "                index = os.path.splitext(os.path.basename(imagem_path))[0]\n",
    "                file_path = os.path.join(f\"{histograms_dir}/{classe}\", f\"{index}_phi_vivos_bgr.pkl\")\n",
    "                joblib.dump(hist_phi_vivos, file_path)\n",
    "                file_path = os.path.join(f\"{histograms_dir}/{classe}\", f\"{index}_phi_mortos_bgr.pkl\")\n",
    "                joblib.dump(hist_phi_mortos, file_path)\n",
    "                file_path = os.path.join(f\"{histograms_dir}/{classe}\", f\"{index}_psi_vivos_bgr.pkl\")\n",
    "                joblib.dump(hist_psi_vivos, file_path)\n",
    "                file_path = os.path.join(f\"{histograms_dir}/{classe}\", f\"{index}_psi_mortos_bgr.pkl\")\n",
    "                joblib.dump(hist_psi_mortos, file_path)\n",
    "            except:\n",
    "                print(\"Erro, analise a pasta de arquivos\")\n",
    "\n",
    "\n",
    "\n",
    "    print(\"Progresso concluído\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beaches\n",
      "bus\n",
      "dinosaurs\n",
      "elephants\n",
      "flowers\n",
      "foods\n",
      "horses\n",
      "monuments\n",
      "mountains_and_snow\n",
      "peolpe_and_villages_in_Africa\n",
      "Progresso concluído\n"
     ]
    }
   ],
   "source": [
    "gerador_histogramas_RGB(dataset_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testando modelos de aprendizado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Carregando histogramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def carrega_hist_rgb(imagem_path):\n",
    "    #O objetivo dessa função é carregar os histogramas de determinada imagem\n",
    "\n",
    "    #data\\dataset_1\\images\\classe\\imagem.jpg\n",
    "    imagem_path = imagem_path.split(\"/\")\n",
    "    classe, imagem = imagem_path[4], imagem_path[5]\n",
    "    imagem = imagem.split(\".\")[0]\n",
    "\n",
    "    \n",
    "    histogramas_dir = f\"{dataset_dir}/histograms_rgb\"\n",
    "    try:\n",
    "        hist_phi_vivos = joblib.load(f\"{histogramas_dir}/{classe}/{imagem}_phi_vivos_bgr.pkl\")\n",
    "        hist_phi_mortos = joblib.load(f\"{histogramas_dir}/{classe}/{imagem}_phi_mortos_bgr.pkl\")\n",
    "        hist_psi_vivos = joblib.load(f\"{histogramas_dir}/{classe}/{imagem}_psi_vivos_bgr.pkl\")\n",
    "        hist_psi_mortos = joblib.load(f\"{histogramas_dir}/{classe}/{imagem}_psi_mortos_bgr.pkl\")\n",
    "\n",
    "        return hist_phi_vivos, hist_phi_mortos, hist_psi_vivos, hist_psi_mortos\n",
    "\n",
    "    except:\n",
    "        print(\"Os histogramas não foram encontrados. Verifique os diretórios e se os histogramas foram criados corretamente.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retorna_combinacao_rgb(imagem_path, combinacao):\n",
    "    # O objetivo dessa funcao é criar combinacoes com os histogramas da imagem\n",
    "\n",
    "    hist_phi_vivos, hist_phi_mortos, hist_psi_vivos, hist_psi_mortos = carrega_hist_rgb(imagem_path)\n",
    "    \n",
    "    switch = {\n",
    "        0: hist_phi_vivos,\n",
    "        1: hist_phi_mortos,\n",
    "        2: hist_psi_vivos,\n",
    "        3: hist_psi_mortos,\n",
    "        4: np.concatenate([hist_phi_vivos, hist_phi_mortos]),   #histogramas phi\n",
    "        5: np.concatenate([hist_psi_vivos, hist_psi_mortos]),   #histogramas psi\n",
    "        6: np.concatenate([hist_phi_vivos, hist_psi_vivos]),    #histogramas vivos\n",
    "        7: np.concatenate([hist_phi_mortos, hist_psi_mortos]),  #histogramas mortos\n",
    "        8: np.concatenate([np.concatenate([hist_phi_vivos, hist_phi_mortos]),\n",
    "                           np.concatenate([hist_psi_vivos, hist_psi_mortos])])\n",
    "    }\n",
    "    \n",
    "    return switch.get(combinacao, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Preparando os dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beaches\n",
      "bus\n",
      "dinosaurs\n",
      "elephants\n",
      "flowers\n",
      "foods\n",
      "horses\n",
      "monuments\n",
      "mountains_and_snow\n",
      "peolpe_and_villages_in_Africa\n",
      "Progresso concluído\n"
     ]
    }
   ],
   "source": [
    "# Prepara os dados para treinar o modelo\n",
    "\n",
    "histogramas_rgb = []\n",
    "rotulos_rgb = []\n",
    "\n",
    "for classe in os.listdir(images_dir):\n",
    "    dir_classe = f\"{images_dir}/{classe}\"\n",
    "    print(classe)\n",
    "    for imagem in os.listdir(dir_classe):\n",
    "        imagem_path = f\"{images_dir}/{classe}/{imagem}\"\n",
    "\n",
    "        #Pode alterar a combinacao \n",
    "        combinacao = retorna_combinacao_rgb(imagem_path, 8)\n",
    "\n",
    "        #Lista dos histogramas\n",
    "        histogramas_rgb.append(combinacao)\n",
    "        #lista da classe assossiada ao histograma\n",
    "        rotulos_rgb.append(classe)\n",
    "\n",
    "print(\"Progresso concluído\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(histogramas_rgb)\n",
    "y = np.array(rotulos_rgb)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Utilizando tecnicas de normalizacao\n",
    "# 1 = MinMaxScaler, 2 = StandardScaler, 3 = MaxAbsScaler, 4 = RobustScaler\n",
    "selectedNormalization = 1\n",
    "\n",
    "if selectedNormalization == 1:\n",
    "    scaler = preprocessing.MinMaxScaler()\n",
    "if selectedNormalization == 2:\n",
    "    scaler = preprocessing.StandardScaler()\n",
    "if selectedNormalization == 3:\n",
    "    scaler = preprocessing.MaxAbsScaler()\n",
    "if selectedNormalization == 4:\n",
    "    scaler = preprocessing.RobustScaler()\n",
    "\n",
    "# Escalando os dados de treinamento\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "# Escalando os dados de teste com os dados de treinamento, visto que os dados de teste podem ser apenas 1 amostra\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testando modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               precision    recall  f1-score   support\n",
      "\n",
      "                      beaches       0.59      0.50      0.54        20\n",
      "                          bus       0.42      0.50      0.45        20\n",
      "                    dinosaurs       1.00      0.05      0.10        20\n",
      "                    elephants       0.39      0.80      0.52        20\n",
      "                      flowers       0.68      0.85      0.76        20\n",
      "                        foods       0.33      0.50      0.40        20\n",
      "                       horses       0.80      0.40      0.53        20\n",
      "                    monuments       0.38      0.30      0.33        20\n",
      "           mountains_and_snow       0.63      0.60      0.62        20\n",
      "peolpe_and_villages_in_Africa       0.65      0.55      0.59        20\n",
      "\n",
      "                     accuracy                           0.51       200\n",
      "                    macro avg       0.59      0.51      0.48       200\n",
      "                 weighted avg       0.59      0.51      0.48       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Gaussian Naive Bayes\n",
    "gnb = GaussianNB()\n",
    "model1 = gnb.fit(X_train, y_train)\n",
    "aux = gnb.predict(X_test)\n",
    "print(classification_report(y_test, aux, zero_division=1))\n",
    "#print(confusion_matrix(y_test, aux))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               precision    recall  f1-score   support\n",
      "\n",
      "                      beaches       0.58      0.35      0.44        20\n",
      "                          bus       0.88      0.75      0.81        20\n",
      "                    dinosaurs       0.95      1.00      0.98        20\n",
      "                    elephants       0.77      0.85      0.81        20\n",
      "                      flowers       0.80      1.00      0.89        20\n",
      "                        foods       0.75      0.75      0.75        20\n",
      "                       horses       0.94      0.85      0.89        20\n",
      "                    monuments       0.72      0.65      0.68        20\n",
      "           mountains_and_snow       0.64      0.80      0.71        20\n",
      "peolpe_and_villages_in_Africa       0.68      0.75      0.71        20\n",
      "\n",
      "                     accuracy                           0.78       200\n",
      "                    macro avg       0.77      0.78      0.77       200\n",
      "                 weighted avg       0.77      0.78      0.77       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "logreg = LogisticRegression(max_iter=1000)\n",
    "model2 = logreg.fit(X_train, y_train)\n",
    "aux = logreg.predict(X_test)\n",
    "print(classification_report(y_test, aux, zero_division=1))\n",
    "#print(confusion_matrix(y_test, aux))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               precision    recall  f1-score   support\n",
      "\n",
      "                      beaches       0.70      0.35      0.47        20\n",
      "                          bus       0.75      0.75      0.75        20\n",
      "                    dinosaurs       0.95      0.95      0.95        20\n",
      "                    elephants       0.65      0.65      0.65        20\n",
      "                      flowers       0.90      0.95      0.93        20\n",
      "                        foods       0.44      0.60      0.51        20\n",
      "                       horses       0.76      0.80      0.78        20\n",
      "                    monuments       0.37      0.50      0.43        20\n",
      "           mountains_and_snow       0.37      0.35      0.36        20\n",
      "peolpe_and_villages_in_Africa       0.67      0.50      0.57        20\n",
      "\n",
      "                     accuracy                           0.64       200\n",
      "                    macro avg       0.66      0.64      0.64       200\n",
      "                 weighted avg       0.66      0.64      0.64       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree\n",
    "dectree = DecisionTreeClassifier()\n",
    "model3 = dectree.fit(X_train, y_train)\n",
    "aux = dectree.predict(X_test)\n",
    "print(classification_report(y_test, aux, zero_division=1))\n",
    "#print(confusion_matrix(y_test, aux))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               precision    recall  f1-score   support\n",
      "\n",
      "                      beaches       0.56      0.45      0.50        20\n",
      "                          bus       0.55      0.30      0.39        20\n",
      "                    dinosaurs       1.00      1.00      1.00        20\n",
      "                    elephants       0.52      0.60      0.56        20\n",
      "                      flowers       0.75      0.90      0.82        20\n",
      "                        foods       0.45      0.65      0.53        20\n",
      "                       horses       0.95      0.95      0.95        20\n",
      "                    monuments       0.35      0.30      0.32        20\n",
      "           mountains_and_snow       0.50      0.15      0.23        20\n",
      "peolpe_and_villages_in_Africa       0.44      0.75      0.56        20\n",
      "\n",
      "                     accuracy                           0.60       200\n",
      "                    macro avg       0.61      0.61      0.59       200\n",
      "                 weighted avg       0.61      0.60      0.59       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# K-Nearest Neighbors\n",
    "knn = KNeighborsClassifier(n_neighbors = 3)\n",
    "model4 = knn.fit(X_train, y_train)\n",
    "aux = knn.predict(X_test)\n",
    "print(classification_report(y_test, aux, zero_division=1))\n",
    "#print(confusion_matrix(y_test, aux))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               precision    recall  f1-score   support\n",
      "\n",
      "                      beaches       0.45      0.25      0.32        20\n",
      "                          bus       0.78      0.70      0.74        20\n",
      "                    dinosaurs       1.00      1.00      1.00        20\n",
      "                    elephants       0.68      0.85      0.76        20\n",
      "                      flowers       0.78      0.90      0.84        20\n",
      "                        foods       0.56      0.50      0.53        20\n",
      "                       horses       0.80      0.80      0.80        20\n",
      "                    monuments       0.67      0.50      0.57        20\n",
      "           mountains_and_snow       0.55      0.55      0.55        20\n",
      "peolpe_and_villages_in_Africa       0.57      0.85      0.68        20\n",
      "\n",
      "                     accuracy                           0.69       200\n",
      "                    macro avg       0.68      0.69      0.68       200\n",
      "                 weighted avg       0.68      0.69      0.68       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Linear Discriminant Analysis\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "model5 = lda.fit(X_train, y_train)\n",
    "aux = lda.predict(X_test)\n",
    "print(classification_report(y_test, aux, zero_division=1))\n",
    "#print(confusion_matrix(y_test, aux))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               precision    recall  f1-score   support\n",
      "\n",
      "                      beaches       0.53      0.45      0.49        20\n",
      "                          bus       0.76      0.80      0.78        20\n",
      "                    dinosaurs       0.95      1.00      0.98        20\n",
      "                    elephants       0.81      0.85      0.83        20\n",
      "                      flowers       0.87      1.00      0.93        20\n",
      "                        foods       0.70      0.70      0.70        20\n",
      "                       horses       1.00      0.95      0.97        20\n",
      "                    monuments       0.54      0.35      0.42        20\n",
      "           mountains_and_snow       0.57      0.65      0.60        20\n",
      "peolpe_and_villages_in_Africa       0.64      0.70      0.67        20\n",
      "\n",
      "                     accuracy                           0.74       200\n",
      "                    macro avg       0.74      0.74      0.74       200\n",
      "                 weighted avg       0.74      0.74      0.74       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Support Vector Machine\n",
    "svm = SVC()\n",
    "model6 = svm.fit(X_train, y_train)\n",
    "aux = svm.predict(X_test)\n",
    "print(classification_report(y_test, aux, zero_division=1))\n",
    "#print(confusion_matrix(y_test, aux))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               precision    recall  f1-score   support\n",
      "\n",
      "                      beaches       0.81      0.65      0.72        20\n",
      "                          bus       0.75      0.75      0.75        20\n",
      "                    dinosaurs       0.87      1.00      0.93        20\n",
      "                    elephants       0.81      0.85      0.83        20\n",
      "                      flowers       0.90      0.90      0.90        20\n",
      "                        foods       0.67      0.80      0.73        20\n",
      "                       horses       0.90      0.95      0.93        20\n",
      "                    monuments       0.76      0.65      0.70        20\n",
      "           mountains_and_snow       0.67      0.60      0.63        20\n",
      "peolpe_and_villages_in_Africa       0.75      0.75      0.75        20\n",
      "\n",
      "                     accuracy                           0.79       200\n",
      "                    macro avg       0.79      0.79      0.79       200\n",
      "                 weighted avg       0.79      0.79      0.79       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# RandomForest\n",
    "rf = RandomForestClassifier()\n",
    "model7 = rf.fit(X_train, y_train)\n",
    "aux = rf.predict(X_test)\n",
    "print(classification_report(y_test, aux, zero_division=1))\n",
    "#print(confusion_matrix(y_test, aux))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               precision    recall  f1-score   support\n",
      "\n",
      "                      beaches       0.67      0.40      0.50        20\n",
      "                          bus       0.88      0.70      0.78        20\n",
      "                    dinosaurs       0.95      1.00      0.98        20\n",
      "                    elephants       0.83      0.75      0.79        20\n",
      "                      flowers       0.83      1.00      0.91        20\n",
      "                        foods       0.81      0.85      0.83        20\n",
      "                       horses       0.95      0.90      0.92        20\n",
      "                    monuments       0.71      0.75      0.73        20\n",
      "           mountains_and_snow       0.67      0.80      0.73        20\n",
      "peolpe_and_villages_in_Africa       0.62      0.75      0.68        20\n",
      "\n",
      "                     accuracy                           0.79       200\n",
      "                    macro avg       0.79      0.79      0.78       200\n",
      "                 weighted avg       0.79      0.79      0.78       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Neural Net\n",
    "nnet = MLPClassifier(alpha=1)\n",
    "model8 = nnet.fit(X_train, y_train)\n",
    "aux = nnet.predict(X_test)\n",
    "print(classification_report(y_test, aux, zero_division=1))\n",
    "#print(confusion_matrix(y_test, aux))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jQy0zeioJZis",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Caltech256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Definindo diretórios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = f\"./data\"\n",
    "\n",
    "#caltech256\n",
    "dataset_1_dir = f\"{data_dir}/dataset_1\"\n",
    "images_1_dir = f\"{dataset_1_dir}/images\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cVmW9D-CJdwd",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Em RBG:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jzGzBaQfJja-"
   },
   "source": [
    "### Gerando histogramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8o9-1DxUfqqd"
   },
   "outputs": [],
   "source": [
    "def obter_vizinhos_rgb(matriz_de_intensidade, linha, coluna):\n",
    "    # O objetivo dessa função é retornar uma lista contendo a intensidade dos 9 vizinhos do pixel observado\n",
    "\n",
    "    #Esse método de partição da matriz foi escolhido para manter a analise dos pixels vizinhos\n",
    "    #dentro dos limites da matriz de intensidade, evitando assim valores negativos ou fora do shape.\n",
    "    vizinhos = matriz_de_intensidade[max(0, linha-1):min(matriz_de_intensidade.shape[0], linha+2),\n",
    "                             max(0, coluna-1):min(matriz_de_intensidade.shape[1], coluna+2)]\n",
    "\n",
    "    #Transforma a matriz particionada em lista e remove o pixel do centro\n",
    "    lista_de_vizinhos = vizinhos.flatten().tolist()\n",
    "    lista_de_vizinhos.remove(matriz_de_intensidade[linha, coluna])\n",
    "\n",
    "    return lista_de_vizinhos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p4YOp87Neg6_"
   },
   "outputs": [],
   "source": [
    "def aplica_regras_rgb(lista_de_vizinhos, intensidade_pixel_central, estado_pixel_central):\n",
    "    #O objetivo dessa função é aplicar as regras do jogo da vida de Conway no pixel observado e retornar seu estado.\n",
    "    # 1 = vivo, 0 = morto\n",
    "\n",
    "    #Conta o número de vizinhos vivos com a mesma intensidade que o pixel central\n",
    "    vizinhos_iguais = lista_de_vizinhos.count(intensidade_pixel_central)\n",
    "\n",
    "    #Regra 1: A célula viva com dois ou três vizinhos vivos sobrevive\n",
    "    if estado_pixel_central == 1 and (vizinhos_iguais == 2 or vizinhos_iguais == 3):\n",
    "        return 1\n",
    "\n",
    "    #Regra 2: A célula viva com menos de dois vizinhos vivos morre (subpopulação)\n",
    "    elif estado_pixel_central == 1 and vizinhos_iguais < 2:\n",
    "        return 0\n",
    "\n",
    "    #Regra 3: A célula viva com mais de três vizinhos vivos morre (superpopulação)\n",
    "    elif estado_pixel_central == 1 and vizinhos_iguais > 3:\n",
    "        return 0\n",
    "\n",
    "    #Regra 4: A célula morta com exatamente três vizinhos vivos se torna viva (resurreição)\n",
    "    elif estado_pixel_central == 0 and vizinhos_iguais == 3:\n",
    "        return 1\n",
    "\n",
    "    #Pra todos os outros casos, a célula permanece no mesmo estado\n",
    "    else:\n",
    "        return estado_pixel_central"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "os6eVfeHebwm"
   },
   "outputs": [],
   "source": [
    "def percorre_imagem_aplicando_regras_rgb(matriz_de_estados, matriz_de_intensidade):\n",
    "    # O objetivo dessa função é percorrer a imagem, chamar a função para obter os vizinhos e aplicar as regras\n",
    "\n",
    "    linhas, colunas = matriz_de_intensidade.shape\n",
    "    for linha in range(linhas):\n",
    "        for coluna in range(colunas):\n",
    "            #Obtem os vizinhos do pixel atual\n",
    "            lista_de_vizinhos = obter_vizinhos_rgb(matriz_de_intensidade, linha, coluna)\n",
    "            #Aplica as regras do jogo da vida no pixel atual (atualiza a matriz de estado inicial)\n",
    "            matriz_de_estados[linha, coluna] = aplica_regras_rgb(lista_de_vizinhos, matriz_de_intensidade[linha, coluna], matriz_de_estados[linha, coluna])\n",
    "\n",
    "    return matriz_de_estados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5PcpNT2neZBn"
   },
   "outputs": [],
   "source": [
    "def gera_histogramas_rgb(imagem):\n",
    "    #O objetivo dessa função é criar as matriz de intensidade e as de estado inicial para cada imagem\n",
    "    #Após aplicar as regras cria os histogramas\n",
    "\n",
    "    #Transforma a imagem em uma matriz de intensidade\n",
    "    matriz_de_intensidade = np.array(imagem)\n",
    "\n",
    "    #Cria as matrizes de estados iniciais\n",
    "    matriz_de_estados_phi = np.ones(matriz_de_intensidade.shape, dtype=int) #todos vivos\n",
    "    matriz_de_estados_psi = np.zeros(matriz_de_intensidade.shape, dtype=int) #todos mortos\n",
    "\n",
    "    #Aplica as regras do jogo da vida e atualiza as matrizes de estado inicial\n",
    "    matriz_de_estados_phi = percorre_imagem_aplicando_regras_rgb(matriz_de_estados_phi, matriz_de_intensidade)\n",
    "    matriz_de_estados_psi = percorre_imagem_aplicando_regras_rgb(matriz_de_estados_psi, matriz_de_intensidade)\n",
    "\n",
    "    #As matrizes são convertidas em listas\n",
    "    #Phi -> estado inicial = vivo\n",
    "    phi_vivos = matriz_de_intensidade.flatten()[matriz_de_estados_phi.flatten() == 1] #se manteram vivos\n",
    "    phi_mortos = matriz_de_intensidade.flatten()[matriz_de_estados_phi.flatten() == 0] #morreram\n",
    "\n",
    "    #Psi -> estado inicial = morto\n",
    "    psi_vivos = matriz_de_intensidade.flatten()[matriz_de_estados_psi.flatten() == 1] #ressuscitaram\n",
    "    psi_mortos = matriz_de_intensidade.flatten()[matriz_de_estados_psi.flatten() == 0] #se manteram mortos\n",
    "\n",
    "    #Cria os histogramas\n",
    "    hist_phi_vivos, _ = np.histogram(phi_vivos, bins=256, range=(0, 256))\n",
    "    hist_phi_mortos, _ = np.histogram(phi_mortos, bins=256, range=(0, 256))\n",
    "    hist_psi_vivos, _ = np.histogram(psi_vivos, bins=256, range=(0, 256))\n",
    "    hist_psi_mortos, _ = np.histogram(psi_mortos, bins=256, range=(0, 256))\n",
    "\n",
    "    return hist_phi_vivos, hist_phi_mortos, hist_psi_vivos, hist_psi_mortos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tVwGT2LUJs-N"
   },
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "\n",
    "def gerador_histogramas_RGB(dataset_dir):\n",
    "    images_dir = f\"{dataset_dir}/images\"\n",
    "    histograms_dir = f\"{dataset_dir}/histograms_rgb\"\n",
    "    os.makedirs(histograms_dir, exist_ok=True)\n",
    "\n",
    "    classes = [d for d in os.listdir(images_dir) if os.path.isdir(os.path.join(images_dir, d))]\n",
    "    for classe in classes:\n",
    "        os.makedirs(os.path.join(histograms_dir, classe), exist_ok=True)\n",
    "\n",
    "    i = 1\n",
    "    for classe in classes:\n",
    "        print(classe)\n",
    "        dir_classe = os.path.join(images_dir, classe)\n",
    "        for imagem in os.listdir(dir_classe):\n",
    "            try:\n",
    "                imagem_path = os.path.join(dir_classe, imagem)\n",
    "                imagem_rgb = cv2.imread(imagem_path, cv2.IMREAD_COLOR)\n",
    "                imagem_rgb = cv2.resize(imagem_rgb, (128, 128))  # Resize para 128x128\n",
    "                \n",
    "                hist_phi_vivos_b, hist_phi_mortos_b, hist_psi_vivos_b, hist_psi_mortos_b = gera_histogramas_rgb(imagem_rgb[:, :, 0])\n",
    "                hist_phi_vivos_g, hist_phi_mortos_g, hist_psi_vivos_g, hist_psi_mortos_g = gera_histogramas_rgb(imagem_rgb[:, :, 1])\n",
    "                hist_phi_vivos_r, hist_phi_mortos_r, hist_psi_vivos_r, hist_psi_mortos_r = gera_histogramas_rgb(imagem_rgb[:, :, 2])\n",
    "                \n",
    "                hist_phi_vivos = np.concatenate([np.concatenate([hist_phi_vivos_b,hist_phi_vivos_g]),hist_phi_vivos_r])\n",
    "                hist_phi_mortos = np.concatenate([np.concatenate([hist_phi_mortos_b,hist_phi_mortos_g]),hist_phi_mortos_r])\n",
    "                hist_psi_vivos = np.concatenate([np.concatenate([hist_psi_vivos_b,hist_psi_vivos_g]),hist_psi_vivos_r])\n",
    "                hist_psi_mortos = np.concatenate([np.concatenate([hist_psi_mortos_b,hist_psi_mortos_g]),hist_psi_mortos_r])\n",
    "                    \n",
    "                index = os.path.splitext(os.path.basename(imagem_path))[0]\n",
    "                file_path = os.path.join(f\"{histograms_dir}/{classe}\", f\"{index}_phi_vivos_bgr.pkl\")\n",
    "                joblib.dump(hist_phi_vivos, file_path)\n",
    "                file_path = os.path.join(f\"{histograms_dir}/{classe}\", f\"{index}_phi_mortos_bgr.pkl\")\n",
    "                joblib.dump(hist_phi_mortos, file_path)\n",
    "                file_path = os.path.join(f\"{histograms_dir}/{classe}\", f\"{index}_psi_vivos_bgr.pkl\")\n",
    "                joblib.dump(hist_psi_vivos, file_path)\n",
    "                file_path = os.path.join(f\"{histograms_dir}/{classe}\", f\"{index}_psi_mortos_bgr.pkl\")\n",
    "                joblib.dump(hist_psi_mortos, file_path)\n",
    "            except:\n",
    "                print(\"Erro, analise a pasta de arquivos\")\n",
    "\n",
    "\n",
    "\n",
    "    print(\"Progresso concluído\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H53ODYudNj40"
   },
   "outputs": [],
   "source": [
    "#gerador_histogramas_RGB(dataset_1_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M7LdASJ9Jnt-",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Testando modelos de aprendizado "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def carrega_hist_rgb(imagem_path):\n",
    "    #O objetivo dessa função é carregar os histogramas de determinada imagem\n",
    "\n",
    "    #data\\dataset_1\\images\\classe\\imagem.jpg\n",
    "    imagem_path = imagem_path.split(\"/\")\n",
    "    classe, imagem = imagem_path[4], imagem_path[5]\n",
    "    imagem = imagem.split(\".\")[0]\n",
    "\n",
    "    \n",
    "    histogramas_dir = f\"{dataset_1_dir}/histograms_rgb\"\n",
    "    try:\n",
    "        hist_phi_vivos = joblib.load(f\"{histogramas_dir}/{classe}/{imagem}_phi_vivos_bgr.pkl\")\n",
    "        hist_phi_mortos = joblib.load(f\"{histogramas_dir}/{classe}/{imagem}_phi_mortos_bgr.pkl\")\n",
    "        hist_psi_vivos = joblib.load(f\"{histogramas_dir}/{classe}/{imagem}_psi_vivos_bgr.pkl\")\n",
    "        hist_psi_mortos = joblib.load(f\"{histogramas_dir}/{classe}/{imagem}_psi_mortos_bgr.pkl\")\n",
    "\n",
    "        return hist_phi_vivos, hist_phi_mortos, hist_psi_vivos, hist_psi_mortos\n",
    "\n",
    "    except:\n",
    "        print(\"Os histogramas não foram encontrados. Verifique os diretórios e se os histogramas foram criados corretamente.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retorna_combinacao_rgb(imagem_path, combinacao):\n",
    "    # O objetivo dessa funcao é criar combinacoes com os histogramas da imagem\n",
    "\n",
    "    hist_phi_vivos, hist_phi_mortos, hist_psi_vivos, hist_psi_mortos = carrega_hist_rgb(imagem_path)\n",
    "    \n",
    "    switch = {\n",
    "        0: hist_phi_vivos,\n",
    "        1: hist_phi_mortos,\n",
    "        2: hist_psi_vivos,\n",
    "        3: hist_psi_mortos,\n",
    "        4: np.concatenate([hist_phi_vivos, hist_phi_mortos]),   #histogramas phi\n",
    "        5: np.concatenate([hist_psi_vivos, hist_psi_mortos]),   #histogramas psi\n",
    "        6: np.concatenate([hist_phi_vivos, hist_psi_vivos]),    #histogramas vivos\n",
    "        7: np.concatenate([hist_phi_mortos, hist_psi_mortos]),  #histogramas mortos\n",
    "        8: np.concatenate([np.concatenate([hist_phi_vivos, hist_phi_mortos]),\n",
    "                           np.concatenate([hist_psi_vivos, hist_psi_mortos])])\n",
    "    }\n",
    "    \n",
    "    return switch.get(combinacao, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 0 -> \"hist_phi_vivos\",       1 -> \"hist_phi_mortos\",   2 -> \"hist_psi_vivos\",         3 -> \"hist_psi_mortos\",\n",
    "## 4 -> \"hist_phi_combined\",    5 -> \"hist_psi_combined\", 6 -> \"hist_vivos_combined\",\n",
    "## 7 -> \"hist_mortos_combined\", 8 -> \"all_combined\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepara os dados para treinar o modelo\n",
    "\n",
    "histogramas_rgb = []\n",
    "rotulos_rgb = []\n",
    "\n",
    "for classe in os.listdir(images_1_dir):\n",
    "    dir_classe = f\"{images_1_dir}/{classe}\"\n",
    "    #print(classe)\n",
    "    for imagem in os.listdir(dir_classe):\n",
    "        imagem_path = f\"{images_1_dir}/{classe}/{imagem}\"\n",
    "\n",
    "        #Pode alterar a combinacao \n",
    "        combinacao = retorna_combinacao_rgb(imagem_path, 8)\n",
    "\n",
    "        #Lista dos histogramas\n",
    "        histogramas_rgb.append(combinacao)\n",
    "        #lista da classe assossiada ao histograma\n",
    "        rotulos_rgb.append(classe)\n",
    "\n",
    "print(\"Progresso concluído\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(histogramas_rgb)\n",
    "y = np.array(rotulos_rgb)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Utilizando tecnicas de normalizacao\n",
    "# 1 = MinMaxScaler, 2 = StandardScaler, 3 = MaxAbsScaler, 4 = RobustScaler\n",
    "selectedNormalization = 1\n",
    "\n",
    "if selectedNormalization == 1:\n",
    "    scaler = preprocessing.MinMaxScaler()\n",
    "if selectedNormalization == 2:\n",
    "    scaler = preprocessing.StandardScaler()\n",
    "if selectedNormalization == 3:\n",
    "    scaler = preprocessing.MaxAbsScaler()\n",
    "if selectedNormalization == 4:\n",
    "    scaler = preprocessing.RobustScaler()\n",
    "\n",
    "# Escalando os dados de treinamento\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "# Escalando os dados de teste com os dados de treinamento, visto que os dados de teste podem ser apenas 1 amostra\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian Naive Bayes\n",
    "gnb = GaussianNB()\n",
    "model1 = gnb.fit(X_train, y_train)\n",
    "aux = gnb.predict(X_test)\n",
    "print(classification_report(y_test, aux, zero_division=1))\n",
    "#print(confusion_matrix(y_test, aux))\n",
    "\n",
    "# acuracia sem scaler   == 0.05\n",
    "# acuriacia com scaler1 == 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "logreg = LogisticRegression()\n",
    "model2 = logreg.fit(X_train, y_train)\n",
    "aux = logreg.predict(X_test)\n",
    "print(classification_report(y_test, aux, zero_division=1))\n",
    "#print(confusion_matrix(y_test, aux))\n",
    "\n",
    "# acuracia sem scaler == 0.11\n",
    "# acuriacia com scaler1 == 0.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree\n",
    "dectree = DecisionTreeClassifier()\n",
    "model3 = dectree.fit(X_train, y_train)\n",
    "aux = dectree.predict(X_test)\n",
    "print(classification_report(y_test, aux, zero_division=1))\n",
    "#print(confusion_matrix(y_test, aux))\n",
    "\n",
    "# acuracia sem scaler   == 0.07\n",
    "# acuriacia com scaler1 == 0.07"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Nearest Neighbors\n",
    "knn = KNeighborsClassifier(n_neighbors = 3)\n",
    "model4 = knn.fit(X_train, y_train)\n",
    "aux = knn.predict(X_test)\n",
    "print(classification_report(y_test, aux, zero_division=1))\n",
    "#print(confusion_matrix(y_test, aux))\n",
    "\n",
    "# acuracia sem scaler   == 0.07\n",
    "# acuriacia com scaler1 == 0.07"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Discriminant Analysis\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "model5 = lda.fit(X_train, y_train)\n",
    "aux = lda.predict(X_test)\n",
    "print(classification_report(y_test, aux, zero_division=1))\n",
    "#print(confusion_matrix(y_test, aux))\n",
    "\n",
    "# acuracia sem scaler   == 0.08\n",
    "# acuriacia com scaler1 == 0.08"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support Vector Machine\n",
    "svm = SVC()\n",
    "model6 = svm.fit(X_train, y_train)\n",
    "aux = svm.predict(X_test)\n",
    "print(classification_report(y_test, aux, zero_division=1))\n",
    "#print(confusion_matrix(y_test, aux))\n",
    "\n",
    "# acuracia sem scaler   == 0.10\n",
    "# acuriacia com scaler1 == 0.12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomForest\n",
    "rf = RandomForestClassifier()\n",
    "model7 = rf.fit(X_train, y_train)\n",
    "aux = rf.predict(X_test)\n",
    "print(classification_report(y_test, aux, zero_division=1))\n",
    "#print(confusion_matrix(y_test, aux))\n",
    "\n",
    "# acuracia sem scaler   == 0.15\n",
    "# acuriacia com scaler1 == 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Net\n",
    "nnet = MLPClassifier(alpha=1)\n",
    "model8 = nnet.fit(X_train, y_train)\n",
    "aux = nnet.predict(X_test)\n",
    "print(classification_report(y_test, aux, zero_division=1))\n",
    "#print(confusion_matrix(y_test, aux))\n",
    "\n",
    "# acuracia sem scaler   == 0.03\n",
    "# acuriacia com scaler1 == 0.07"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Em escala de cinza:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Gerando histogramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obter_vizinhos(matriz_de_intensidade, linha, coluna):\n",
    "    # O objetivo dessa função é retornar uma lista contendo a intensidade dos 9 vizinhos do pixel observado\n",
    "\n",
    "    #Esse método de partição da matriz foi escolhido para manter a analise dos pixels vizinhos\n",
    "    #dentro dos limites da matriz de intensidade, evitando assim valores negativos ou fora do shape.\n",
    "    vizinhos = matriz_de_intensidade[max(0, linha-1):min(matriz_de_intensidade.shape[0], linha+2),\n",
    "                             max(0, coluna-1):min(matriz_de_intensidade.shape[1], coluna+2)]\n",
    "\n",
    "    #Transforma a matriz particionada em lista e remove o pixel do centro\n",
    "    lista_de_vizinhos = vizinhos.flatten().tolist()\n",
    "    lista_de_vizinhos.remove(matriz_de_intensidade[linha, coluna])\n",
    "\n",
    "    return lista_de_vizinhos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aplica_regras(lista_de_vizinhos, intensidade_pixel_central, estado_pixel_central):\n",
    "    #O objetivo dessa função é aplicar as regras do jogo da vida de Conway no pixel observado e retornar seu estado.\n",
    "    # 1 = vivo, 0 = morto\n",
    "\n",
    "    #Conta o número de vizinhos vivos com a mesma intensidade que o pixel central\n",
    "    vizinhos_iguais = lista_de_vizinhos.count(intensidade_pixel_central)\n",
    "\n",
    "    #Regra 1: A célula viva com dois ou três vizinhos vivos sobrevive\n",
    "    if estado_pixel_central == 1 and (vizinhos_iguais == 2 or vizinhos_iguais == 3):\n",
    "        return 1\n",
    "\n",
    "    #Regra 2: A célula viva com menos de dois vizinhos vivos morre (subpopulação)\n",
    "    elif estado_pixel_central == 1 and vizinhos_iguais < 2:\n",
    "        return 0\n",
    "\n",
    "    #Regra 3: A célula viva com mais de três vizinhos vivos morre (superpopulação)\n",
    "    elif estado_pixel_central == 1 and vizinhos_iguais > 3:\n",
    "        return 0\n",
    "\n",
    "    #Regra 4: A célula morta com exatamente três vizinhos vivos se torna viva (resurreição)\n",
    "    elif estado_pixel_central == 0 and vizinhos_iguais == 3:\n",
    "        return 1\n",
    "\n",
    "    #Pra todos os outros casos, a célula permanece no mesmo estado\n",
    "    else:\n",
    "        return estado_pixel_central"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def percorre_imagem_aplicando_regras(matriz_de_estados, matriz_de_intensidade):\n",
    "    # O objetivo dessa função é percorrer a imagem, chamar a função para obter os vizinhos e aplicar as regras\n",
    "\n",
    "    linhas, colunas = matriz_de_intensidade.shape\n",
    "    for linha in range(linhas):\n",
    "        for coluna in range(colunas):\n",
    "            #Obtem os vizinhos do pixel atual\n",
    "            lista_de_vizinhos = obter_vizinhos(matriz_de_intensidade, linha, coluna)\n",
    "            #Aplica as regras do jogo da vida no pixel atual (atualiza a matriz de estado inicial)\n",
    "            matriz_de_estados[linha, coluna] = aplica_regras(lista_de_vizinhos, matriz_de_intensidade[linha, coluna], matriz_de_estados[linha, coluna])\n",
    "\n",
    "    return matriz_de_estados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gera_histogramas(imagem_cinza):\n",
    "    #O objetivo dessa função é criar as matriz de intensidade e as de estado inicial para cada imagem\n",
    "    #Após aplicar as regras cria os histogramas\n",
    "\n",
    "    #Transforma a imagem em uma matriz de intensidade\n",
    "    matriz_de_intensidade = np.array(imagem_cinza)\n",
    "\n",
    "    #Cria as matrizes de estados iniciais\n",
    "    matriz_de_estados_phi = np.ones(matriz_de_intensidade.shape, dtype=int) #todos vivos\n",
    "    matriz_de_estados_psi = np.zeros(matriz_de_intensidade.shape, dtype=int) #todos mortos\n",
    "\n",
    "    #Aplica as regras do jogo da vida e atualiza as matrizes de estado inicial\n",
    "    matriz_de_estados_phi = percorre_imagem_aplicando_regras(matriz_de_estados_phi, matriz_de_intensidade)\n",
    "    matriz_de_estados_psi = percorre_imagem_aplicando_regras(matriz_de_estados_psi, matriz_de_intensidade)\n",
    "\n",
    "    #As matrizes são convertidas em listas\n",
    "    #Phi -> estado inicial = vivo\n",
    "    phi_vivos = matriz_de_intensidade.flatten()[matriz_de_estados_phi.flatten() == 1] #se manteram vivos\n",
    "    phi_mortos = matriz_de_intensidade.flatten()[matriz_de_estados_phi.flatten() == 0] #morreram\n",
    "\n",
    "    #Psi -> estado inicial = morto\n",
    "    psi_vivos = matriz_de_intensidade.flatten()[matriz_de_estados_psi.flatten() == 1] #ressuscitaram\n",
    "    psi_mortos = matriz_de_intensidade.flatten()[matriz_de_estados_psi.flatten() == 0] #se manteram mortos\n",
    "\n",
    "    #Cria os histogramas\n",
    "    hist_phi_vivos, _ = np.histogram(phi_vivos, bins=256, range=(0, 256))\n",
    "    hist_phi_mortos, _ = np.histogram(phi_mortos, bins=256, range=(0, 256))\n",
    "    hist_psi_vivos, _ = np.histogram(psi_vivos, bins=256, range=(0, 256))\n",
    "    hist_psi_mortos, _ = np.histogram(psi_mortos, bins=256, range=(0, 256))\n",
    "\n",
    "    return hist_phi_vivos, hist_phi_mortos, hist_psi_vivos, hist_psi_mortos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gerador_histogramas(dataset_dir):\n",
    "    #Funcao principal: faz a chamada das funções acima e salva os histogramas\n",
    "\n",
    "    images_dir = f\"{dataset_dir}/images\"\n",
    "    histograms_dir = f\"{dataset_dir}/histograms_grayscale\"\n",
    "\n",
    "    # Cria as pastas para salvar os histogramas mantendo o padrão das classes do dataset\n",
    "    os.makedirs(histograms_dir, exist_ok=True)\n",
    "\n",
    "    classes = [conteudo_item for conteudo_item in os.listdir(images_dir) if os.path.isdir(os.path.join(images_dir, conteudo_item))]\n",
    "    for classe in classes:\n",
    "        dir_pastas = os.path.join(histograms_dir, classe)\n",
    "        os.makedirs(dir_pastas, exist_ok=True)\n",
    "\n",
    "    i = 0\n",
    "    #Esse loop pega a imagem, gera seus histogramas e salva com base no nome da classe e da imagem\n",
    "    for classe in os.listdir(images_dir):\n",
    "        dir_classe = f\"{images_dir}/{classe}\"\n",
    "        \n",
    "        i += 1\n",
    "        if i <= 175:\n",
    "            pass\n",
    "        else:\n",
    "            print(classe)\n",
    "            for imagem in os.listdir(dir_classe):\n",
    "                imagem_path = f\"{images_dir}/{classe}/{imagem}\"\n",
    "            \n",
    "                try:\n",
    "                    imagem_cinza = cv2.imread(imagem_path, cv2.IMREAD_GRAYSCALE)\n",
    "        \n",
    "                    imagem_cinza = cv2.resize(imagem_cinza, (128,128))\n",
    "                    #Dimensao original = 384 * 256 = 98304\n",
    "                    #Resize = 128 * 128 = 16384\n",
    "                    #Redução = (16384-98304)/98304*100 = 83,33%\n",
    "        \n",
    "                    hist_phi_vivos, hist_phi_mortos, hist_psi_vivos, hist_psi_mortos = gera_histogramas(imagem_cinza)\n",
    "        \n",
    "                    index = imagem.split(\".\")[0]\n",
    "                    file_path = os.path.join(f\"{histograms_dir}/{classe}\", f\"{index}_phi_vivos.pkl\")\n",
    "                    joblib.dump(hist_phi_vivos, file_path)\n",
    "                    file_path = os.path.join(f\"{histograms_dir}/{classe}\", f\"{index}_phi_mortos.pkl\")\n",
    "                    joblib.dump(hist_phi_mortos, file_path)\n",
    "                    file_path = os.path.join(f\"{histograms_dir}/{classe}\", f\"{index}_psi_vivos.pkl\")\n",
    "                    joblib.dump(hist_psi_vivos, file_path)\n",
    "                    file_path = os.path.join(f\"{histograms_dir}/{classe}\", f\"{index}_psi_mortos.pkl\")\n",
    "                    joblib.dump(hist_psi_mortos, file_path)\n",
    "                except:\n",
    "                    print(\"Um erro foi encontrado, revise o dataset\")\n",
    "\n",
    "    print(\"Progresso concluído\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gerador_histogramas(dataset_1_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Testando modelos de aprendizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def carrega_hist(imagem_path):\n",
    "    #O objetivo dessa função é carregar os histogramas de determinada imagem\n",
    "\n",
    "    #data\\dataset\\images\\beaches\\100.jpg\n",
    "    imagem_path = imagem_path.split(\"/\")\n",
    "    classe, imagem = imagem_path[4], imagem_path[5]\n",
    "    imagem = imagem.split(\".\")[0]\n",
    "\n",
    "    i = 0\n",
    "    histogramas_dir = f\"{dataset_1_dir}/histograms_grayscale\"\n",
    "    try:\n",
    "        i+=1\n",
    "        hist_phi_vivos = joblib.load(f\"{histogramas_dir}/{classe}/{imagem}_phi_vivos.pkl\")\n",
    "        hist_phi_mortos = joblib.load(f\"{histogramas_dir}/{classe}/{imagem}_phi_mortos.pkl\")\n",
    "        hist_psi_vivos = joblib.load(f\"{histogramas_dir}/{classe}/{imagem}_psi_vivos.pkl\")\n",
    "        hist_psi_mortos = joblib.load(f\"{histogramas_dir}/{classe}/{imagem}_psi_mortos.pkl\")\n",
    "\n",
    "        return hist_phi_vivos, hist_phi_mortos, hist_psi_vivos, hist_psi_mortos\n",
    "\n",
    "    except:\n",
    "        print(\"Os histogramas não foram encontrados. Verifique os diretórios e se os histogramas foram criados corretamente.\")\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retorna_combinacao(imagem_path, combinacao):\n",
    "    # O objetivo dessa funcao é criar combinacoes com os histogramas da imagem\n",
    "\n",
    "    hist_phi_vivos, hist_phi_mortos, hist_psi_vivos, hist_psi_mortos = carrega_hist(imagem_path)\n",
    "    if combinacao == 0:\n",
    "        return hist_phi_vivos\n",
    "    elif combinacao == 1:\n",
    "        return hist_phi_mortos\n",
    "    elif combinacao == 2:\n",
    "        return hist_psi_vivos\n",
    "    elif combinacao == 3:\n",
    "        return hist_psi_mortos\n",
    "    elif combinacao == 4:\n",
    "        return np.concatenate([hist_phi_vivos, hist_phi_mortos]) #histogramas phi\n",
    "    elif combinacao == 5:\n",
    "        return np.concatenate([hist_psi_vivos, hist_psi_mortos]) #histogramas psi\n",
    "    elif combinacao == 6:\n",
    "        return np.concatenate([hist_phi_vivos, hist_psi_vivos]) #histogramas vivos\n",
    "    elif combinacao == 7:\n",
    "        return np.concatenate([hist_phi_mortos, hist_psi_mortos]) #histogramas mortos\n",
    "    elif combinacao == 8:\n",
    "        return np.concatenate([np.concatenate([hist_phi_vivos, hist_phi_mortos]),\n",
    "                               np.concatenate([hist_psi_vivos, hist_psi_mortos])])\n",
    "        #histogramas phi e psi combinados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepara os dados para treinar o modelo\n",
    "\n",
    "histogramas = []\n",
    "rotulos = []\n",
    "\n",
    "for classe in os.listdir(images_1_dir):\n",
    "    dir_classe = f\"{images_1_dir}/{classe}\"\n",
    "    print(classe)\n",
    "    for imagem in os.listdir(dir_classe):\n",
    "        imagem_path = f\"{images_1_dir}/{classe}/{imagem}\"\n",
    "\n",
    "        #Pode alterar a combinacao\n",
    "        combinacao = retorna_combinacao(imagem_path, 8)\n",
    "\n",
    "        #Lista dos histogramas\n",
    "        histogramas.append(combinacao)\n",
    "        #lista da classe assossiada ao histograma\n",
    "        rotulos.append(classe)\n",
    "\n",
    "print(\"Progresso concluído\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(histogramas)\n",
    "y = np.array(rotulos)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1,stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Utilizando tecnicas de normalizacao\n",
    "# 1 = MinMaxScaler, 2 = StandardScaler, 3 = MaxAbsScaler, 4 = RobustScaler\n",
    "selectedNormalization = 1\n",
    "\n",
    "if selectedNormalization == 1:\n",
    "    scaler = preprocessing.MinMaxScaler()\n",
    "if selectedNormalization == 2:\n",
    "    scaler = preprocessing.StandardScaler()\n",
    "if selectedNormalization == 3:\n",
    "    scaler = preprocessing.MaxAbsScaler()\n",
    "if selectedNormalization == 4:\n",
    "    scaler = preprocessing.RobustScaler()\n",
    "\n",
    "# Escalando os dados de treinamento\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "# Escalando os dados de teste com os dados de treinamento, visto que os dados de teste podem ser apenas 1 amostra\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian Naive Bayes\n",
    "gnb = GaussianNB()\n",
    "model1 = gnb.fit(X_train, y_train)\n",
    "aux = gnb.predict(X_test)\n",
    "print(classification_report(y_test, aux, zero_division=1))\n",
    "#print(confusion_matrix(y_test, aux))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "logreg = LogisticRegression()\n",
    "model2 = logreg.fit(X_train, y_train)\n",
    "aux = logreg.predict(X_test)\n",
    "print(classification_report(y_test, aux, zero_division=1))\n",
    "#print(confusion_matrix(y_test, aux))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree\n",
    "dectree = DecisionTreeClassifier()\n",
    "model3 = dectree.fit(X_train, y_train)\n",
    "aux = dectree.predict(X_test)\n",
    "print(classification_report(y_test, aux, zero_division=1))\n",
    "#print(confusion_matrix(y_test, aux))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Nearest Neighbors\n",
    "knn = KNeighborsClassifier(n_neighbors = 3)\n",
    "model4 = knn.fit(X_train, y_train)\n",
    "aux = knn.predict(X_test)\n",
    "print(classification_report(y_test, aux, zero_division=1))\n",
    "#print(confusion_matrix(y_test, aux))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Discriminant Analysis\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "model5 = lda.fit(X_train, y_train)\n",
    "aux = lda.predict(X_test)\n",
    "print(classification_report(y_test, aux, zero_division=1))\n",
    "#print(confusion_matrix(y_test, aux))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support Vector Machine\n",
    "svm = SVC()\n",
    "model6 = svm.fit(X_train, y_train)\n",
    "aux = svm.predict(X_test)\n",
    "print(classification_report(y_test, aux, zero_division=1))\n",
    "#print(confusion_matrix(y_test, aux))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomForest\n",
    "rf = RandomForestClassifier()\n",
    "model7 = rf.fit(X_train, y_train)\n",
    "aux = rf.predict(X_test)\n",
    "print(classification_report(y_test, aux, zero_division=1))\n",
    "#print(confusion_matrix(y_test, aux))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Net\n",
    "nnet = MLPClassifier(alpha=1)\n",
    "model8 = nnet.fit(X_train, y_train)\n",
    "aux = nnet.predict(X_test)\n",
    "print(classification_report(y_test, aux, zero_division=1))\n",
    "#print(confusion_matrix(y_test, aux))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "fDQfybOh1Ucj",
    "3o3UPHAE4aWR"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
